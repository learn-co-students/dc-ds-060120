{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Prediction with LSTMs\n",
    "\n",
    "Adapted from https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load data locally\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download data\n",
    "import requests\n",
    "resp = requests.get('https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text = resp.content.decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or heart, the arm our soldier,\n",
      "Our steed the leg, the tongue our trumpeter.\n",
      "With other muniments and petty helps\n",
      "In this our fabric, if that they--\n",
      "\n",
      "MENENIUS:\n",
      "What then?\n",
      "'Fore me, this fellow speaks! What then? what then?\n",
      "\n",
      "First Citizen:\n",
      "Should by th\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[5000:5250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char in list(char2idx)[:20]:\n",
    "    print(f'  {repr(char):4}: {char2idx[char]:3},')\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "********\n",
      "\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n",
      "********\n",
      "\n",
      "now Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us ki\n",
      "********\n",
      "\n",
      "ll him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be d\n",
      "********\n",
      "\n",
      "one: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citi\n",
      "********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(''.join(idx2char[item.numpy()]))\n",
    "    print('********\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,\n",
    "                                    batch_input_shape=[BATCH_SIZE, None]))\n",
    "model.add(GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'))\n",
    "model.add(Dense(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 100, 65), dtype=float32, numpy=\n",
       "array([[[ 6.83475425e-03,  2.17568353e-02,  1.71400351e-03, ...,\n",
       "         -7.00535858e-03,  3.54460347e-03,  1.15146162e-03],\n",
       "        [ 3.50874127e-03,  2.02313829e-02,  3.04970611e-03, ...,\n",
       "         -3.04260813e-02,  1.45373726e-02,  2.28959247e-02],\n",
       "        [ 9.36156488e-04,  1.45831192e-02,  3.22035304e-03, ...,\n",
       "         -4.45644893e-02,  2.09487937e-02,  3.68625335e-02],\n",
       "        ...,\n",
       "        [ 3.73769808e-03,  1.23782391e-02,  3.54287121e-03, ...,\n",
       "         -2.60768179e-02, -1.91652973e-03,  1.56679600e-02],\n",
       "        [ 1.33352149e-02,  1.91539489e-02, -6.11232780e-03, ...,\n",
       "         -7.11251842e-03, -2.11983942e-03,  2.64493767e-02],\n",
       "        [-2.40197051e-02,  3.27490480e-03,  3.89770372e-03, ...,\n",
       "         -1.12645398e-03,  1.22057823e-02,  6.83099031e-03]],\n",
       "\n",
       "       [[ 1.31722949e-02,  3.88889574e-03, -1.55147794e-03, ...,\n",
       "         -1.21655958e-02,  1.12607190e-02,  3.25414166e-03],\n",
       "        [-2.93978211e-03, -7.73710571e-03,  1.29669043e-03, ...,\n",
       "         -1.35511458e-02,  1.37424264e-02, -8.71318299e-03],\n",
       "        [ 5.96504239e-03,  1.66831128e-02, -1.95665308e-03, ...,\n",
       "         -1.84595082e-02,  1.21165449e-02, -2.01533688e-03],\n",
       "        ...,\n",
       "        [ 1.05395168e-02,  5.56149380e-03,  9.69902053e-03, ...,\n",
       "         -2.90604178e-02,  1.20856566e-02,  1.55728003e-02],\n",
       "        [ 2.10267329e-03,  1.79292522e-02, -1.47921899e-02, ...,\n",
       "         -2.12054402e-02,  3.64542892e-03,  5.85104851e-03],\n",
       "        [-1.26014715e-02,  1.31891500e-02, -3.18360925e-02, ...,\n",
       "         -4.02762517e-02, -6.61011878e-03, -1.02344183e-02]],\n",
       "\n",
       "       [[ 9.15732048e-03,  2.69118068e-03,  4.77838423e-03, ...,\n",
       "         -1.16392337e-02, -4.91750566e-03,  5.30838780e-03],\n",
       "        [-4.51710913e-03, -9.86341108e-03, -2.35262327e-02, ...,\n",
       "         -9.99497715e-03, -2.84103584e-03,  4.21388540e-03],\n",
       "        [ 4.20381920e-03, -1.34370371e-03,  8.35052226e-03, ...,\n",
       "         -2.57189628e-02, -2.86275730e-03, -2.87717301e-03],\n",
       "        ...,\n",
       "        [-3.95397320e-02, -1.04950480e-02,  1.66318528e-02, ...,\n",
       "         -1.13367205e-02,  1.34778386e-02,  1.37085188e-03],\n",
       "        [-1.79714076e-02, -2.02445267e-03,  2.86977347e-02, ...,\n",
       "         -2.77045276e-02,  4.94127395e-03, -2.33280193e-03],\n",
       "        [-2.57697655e-03,  4.19266755e-04,  1.58902984e-02, ...,\n",
       "         -2.84430180e-02,  2.87026213e-03,  3.69944610e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 8.30926280e-03, -3.48304689e-04, -5.71125606e-03, ...,\n",
       "          1.64561551e-02,  1.85973868e-02,  1.37857050e-02],\n",
       "        [ 1.06809307e-02,  4.32742899e-03, -1.47353206e-03, ...,\n",
       "         -3.78140947e-03,  2.78941612e-03,  1.28413122e-02],\n",
       "        [ 1.08928941e-02,  2.12699324e-02,  9.32399926e-05, ...,\n",
       "         -1.18841836e-02,  9.45263484e-04,  5.82369138e-03],\n",
       "        ...,\n",
       "        [ 1.21521112e-02, -1.01245744e-02,  2.26415088e-03, ...,\n",
       "         -2.66152024e-02,  8.93150643e-03,  7.09914323e-03],\n",
       "        [ 1.05303992e-02, -1.08792419e-02, -6.18549297e-03, ...,\n",
       "         -1.12099322e-02,  1.90299451e-02,  1.63217001e-02],\n",
       "        [-7.09112617e-04, -1.44423768e-02,  1.25725506e-04, ...,\n",
       "         -1.68551803e-02,  1.31087527e-02, -1.14867277e-03]],\n",
       "\n",
       "       [[-1.62821021e-02,  7.22726062e-03, -2.48247273e-02, ...,\n",
       "         -2.47769840e-02, -1.13062002e-02, -1.64435711e-02],\n",
       "        [-2.93323547e-02, -2.07593143e-02, -4.77857422e-03, ...,\n",
       "         -2.72095650e-02,  9.10129584e-03, -1.09146722e-02],\n",
       "        [-5.44480467e-03, -1.03720874e-02, -1.11000314e-02, ...,\n",
       "         -3.32607795e-03,  2.15022936e-02,  5.61883021e-03],\n",
       "        ...,\n",
       "        [-3.81502532e-03, -9.61873867e-03,  1.33811627e-02, ...,\n",
       "         -3.02660335e-02,  7.11058127e-03,  7.99667090e-04],\n",
       "        [ 1.25499908e-02, -8.78503080e-03,  1.03652049e-02, ...,\n",
       "         -2.47859173e-02,  9.76649299e-03,  7.17498362e-04],\n",
       "        [ 1.32568143e-02, -3.18519142e-03,  2.15394311e-02, ...,\n",
       "         -3.95012759e-02,  3.33174295e-03, -1.63194386e-03]],\n",
       "\n",
       "       [[ 1.31722949e-02,  3.88889574e-03, -1.55147794e-03, ...,\n",
       "         -1.21655958e-02,  1.12607190e-02,  3.25414166e-03],\n",
       "        [ 8.96108057e-03,  2.09213421e-02, -3.65134468e-03, ...,\n",
       "         -1.58184674e-02,  1.12498105e-02,  3.78143275e-03],\n",
       "        [ 2.28071329e-03,  1.86156873e-02, -2.40185694e-03, ...,\n",
       "         -3.62703390e-02,  1.91895347e-02,  2.43775882e-02],\n",
       "        ...,\n",
       "        [-7.75037659e-03, -1.34798763e-02,  5.90121420e-03, ...,\n",
       "         -2.19442099e-02,  1.33989407e-02,  1.21488441e-02],\n",
       "        [ 6.61999732e-03, -2.88287480e-03,  1.58408144e-03, ...,\n",
       "         -3.05011719e-02,  1.61109678e-02,  1.13754123e-02],\n",
       "        [ 3.75512126e-03,  1.78682934e-02, -1.52812689e-03, ...,\n",
       "         -2.87365913e-02,  1.21079367e-02,  9.25401784e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 256)           394752    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 65)            16705     \n",
      "=================================================================\n",
      "Total params: 428,097\n",
      "Trainable params: 428,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 29, 20, 52,  2, 44, 30, 13, 60, 12, 62, 52,  6, 15, 39, 14, 44,\n",
       "       40, 30, 38, 31, 54, 13, 36, 14, 39, 42, 51, 34, 28, 46, 56, 49, 12,\n",
       "        1, 26, 54, 43, 55, 38, 22, 48, 63, 58, 10, 35, 54, 28, 61, 23, 50,\n",
       "       20, 45, 54, 40,  2, 55, 33,  4, 53,  7, 35, 27, 25, 47, 55, 47, 17,\n",
       "        0, 55,  8,  7,  2,  1, 40,  8, 11, 20, 54, 23, 32, 64, 10, 16, 46,\n",
       "       13, 47, 42, 55,  1, 55, 63, 48, 46, 36, 56, 48, 30,  0, 19])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"hee with hate,\\nDrawn tuns of blood out of thy country's breast,\\nAnd cannot live but to thy shame, un\"\n",
      "\n",
      "Next Char Predictions: \n",
      " 'TQHn!fRAv?xn,CaBfbRZSpAXBadmVPhrk? NpeqZJjyt:WpPwKlHgpb!qU&o-WOMiqiE\\nq.-! b.;HpKTz:DhAidq qyjhXrjR\\nG'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1753197\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 172 steps\n",
      "Epoch 1/10\n",
      "172/172 [==============================] - 39s 227ms/step - loss: 2.6902\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 38s 223ms/step - loss: 2.0858\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 41s 239ms/step - loss: 1.8631\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 40s 231ms/step - loss: 1.7275\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 46s 269ms/step - loss: 1.6395\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 43s 248ms/step - loss: 1.5781\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 40s 234ms/step - loss: 1.5344\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 40s 235ms/step - loss: 1.5008\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 39s 228ms/step - loss: 1.4761\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 39s 227ms/step - loss: 1.4545\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                  ckpt_5.data-00000-of-00001\r\n",
      "ckpt_1.data-00000-of-00001  ckpt_5.index\r\n",
      "ckpt_1.index                ckpt_6.data-00000-of-00001\r\n",
      "ckpt_10.data-00000-of-00001 ckpt_6.index\r\n",
      "ckpt_10.index               ckpt_7.data-00000-of-00001\r\n",
      "ckpt_2.data-00000-of-00001  ckpt_7.index\r\n",
      "ckpt_2.index                ckpt_8.data-00000-of-00001\r\n",
      "ckpt_3.data-00000-of-00001  ckpt_8.index\r\n",
      "ckpt_3.index                ckpt_9.data-00000-of-00001\r\n",
      "ckpt_4.data-00000-of-00001  ckpt_9.index\r\n",
      "ckpt_4.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls training_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 256)            394752    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 65)             16705     \n",
      "=================================================================\n",
      "Total params: 428,097\n",
      "Trainable params: 428,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 2\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "  \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "  \n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "  \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "  \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN SOLO: I'll rat. heatus,\n",
      "TiClY REY:\n",
      "Nipencle, beyks' mose Abouth,y\n",
      "WHetchookim! HaSZREY:\n",
      "WySK:\n",
      "The Sowary give--genu ofe'll live: I'll even-roy, he\n",
      "neprey'd knoct a\n",
      "yebqueb.\n",
      "Whuenden wavagliney kne,\n",
      "wh sin: Grayi.''e\n",
      "Greefulp's reclippige.'Tu lio younatoms.,-y uI\n",
      "\n",
      "ELdBever'tw CAMquazOPRY:\n",
      "Nap. Jatf-Happy, ersm.\n",
      "MaRCALETES:\n",
      "Whyoug cozmanly, 'PePPstrades Romi;\n",
      "bencedest, goubflip-dy.\n",
      "sVoeds! fom-sequirx'dry offord.\n",
      "Homaliva, fity grasw-hidkent 'gpokn flooth\n",
      "Noken weech!--Gut, Wgerchemoly,\n",
      "I'll labood abouch! Om,\n",
      "Sthe ecvess'd.ON&BEEH:\n",
      "He sloy'd-hence?\n",
      "Ozs, a\n",
      "eagenea;\n",
      "Mysseft! would'st kmoke YORY: tyRINGXSRUCH:\n",
      "Forget VyVile.\n",
      "Phevidess'ff.\n",
      "D no is,.\n",
      "Gyen inked SacoGO NI\n",
      "Grovidmatiposeo,\n",
      "Unomey, so. Mhgides hove hist;\n",
      "Foll, yourlowarxes serD.' Likes nay!' LaULIUCL:\n",
      "YOiK:\n",
      "Tearby cupn'\n",
      "Mistechby ou getropyou, back nequr.\n",
      "\n",
      "DUKE!!\n",
      "Welch bowel; RaQLICIOI:-wa!\n",
      "Orqum, behof, folse trrehol mysealls'\n",
      "CyrBYo! 'SquiebroY;--\n",
      "\n",
      "ALfAMll!!\n",
      "I'ClxORUD:\n",
      "Johg-Hadizag; celsher 'CUpo. Colta.\n",
      "If and effecclish. I,ack t\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"HAN SOLO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
