{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/text-miners.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is text mining different? What is text?\n",
    "\n",
    "- Order the words from **SMALLEST** to **LARGEST** units\n",
    " - character\n",
    " - corpora\n",
    " - sentence\n",
    " - word\n",
    " - corpus\n",
    " - paragraph\n",
    " - document\n",
    "\n",
    "(after it is all organized)\n",
    "\n",
    "- Any disagreements about the terms used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tokenization](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\n",
    "### start small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_test = \"Here is a sentence. Or two, I don't think there will be more.\"\n",
    "token_test_2 = \"i thought this sentence was good.\"\n",
    "token_test_3 = \"Here's a sentence... maybe two. Depending on how you like to count!\"\n",
    "test_4 = 'Dr. Strange will see you now. He\\' in the office'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Here's a sentence\", 'maybe two', 'Depending on how you like to count!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's tokenize a document... into sentences\n",
    "def make_sentences(doc):\n",
    "    sentences = doc.split('.')\n",
    "    return [s.strip() for s in sentences if s]\n",
    "\n",
    "make_sentences(token_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "new_token = token_test_3.translate(str.maketrans('', '', string.punctuation))\n",
    "new_token.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Here's\",\n",
       " 'a',\n",
       " 'sentence...',\n",
       " 'maybe',\n",
       " 'two.',\n",
       " 'Depending',\n",
       " 'on',\n",
       " 'how',\n",
       " 'you',\n",
       " 'like',\n",
       " 'to',\n",
       " 'count!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# let's tokenize a document into words\n",
    "# with these 3 test cases what would you look out for?\n",
    "def tokenize_it(doc):\n",
    "    return doc.split(' ')\n",
    "\n",
    "tokenize_it(token_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New library!\n",
    "\n",
    "while we have seen language processing tools in spark, NLTK is its own python library. And of course, it has its own [documentation](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigger Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get('http://www.gutenberg.org/cache/epub/5200/pg5200.txt')\n",
    "metamorph = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\r\n",
      "Translated by David Wyllie.\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.net\r\n",
      "\r\n",
      "** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\r\n",
      "**     Please follow the copyright guidelines in this file.     **\r\n",
      "\r\n",
      "\r\n",
      "Title: Metamorphosis\r\n",
      "\r\n",
      "Author: Franz Kafka\r\n",
      "\r\n",
      "Translator: David Wyllie\r\n",
      "\r\n",
      "Release Date: August 16, 2005 [EBook #5200]\r\n",
      "First posted: May 13, 2002\r\n",
      "Last updated: May 20, 2012\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "\r\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Copyright (C) 2002 David Wyllie.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "  Metamorphosis\r\n",
      "  Franz Kafka\r\n",
      "\r\n",
      "Translated by David Wyllie\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "I\r\n",
      "\r\n",
      "\r\n",
      "One morning, when Gregor Samsa woke from troubled dreams, he found\r\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "print(metamorph[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your article here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\" ##what is this non-sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Regex playground](https://regexr.com/)\n",
    "\n",
    "[Regex resources](https://www.programiz.com/python-programming/regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', 'EBook', 'First', 'posted', 'May', 'Last', 'updated', 'May', 'Language', 'English', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', 'Copyright', 'C', 'David', 'Wyllie', 'Metamorphosis', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'I', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', \"What's\", 'happened', 'to', 'me', 'he', 'thought', 'It', \"wasn't\", 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'its', 'four', 'familiar', 'walls', 'A', 'collection', 'of', 'textile', 'samples', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'was', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', 'gilded', 'frame', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather', 'Drops', 'of', 'rain', 'could', 'be', 'heard', 'hitting', 'the', 'pane', 'which', 'made', 'him', 'feel', 'quite', 'sad', 'How', 'about', 'if', 'I', 'sleep', 'a', 'little', 'bit', 'longer', 'and', 'forget', 'all', 'this', 'nonsense', 'he', 'thought', 'but', 'that', 'was', 'something', 'he', 'was', 'unable', 'to', 'do', 'because', 'he', 'was', 'used', 'to', 'sleeping', 'on', 'his', 'right', 'and', 'in', 'his', 'present', 'state', \"couldn't\", 'get', 'into', 'that', 'position', 'However', 'hard', 'he', 'threw', 'himself', 'onto', 'his', 'right', 'he', 'always', 'rolled', 'back', 'to', 'where', 'he', 'was', 'He', 'must', 'have', 'tried', 'it', 'a', 'hundred', 'times', 'shut', 'his', 'eyes', 'so', 'that', 'he', \"wouldn't\", 'have', 'to', 'look', 'at', 'the', 'floundering', 'legs', 'and', 'only', 'stopped', 'when', 'he', 'began', 'to', 'feel', 'a', 'mild', 'dull', 'pain', 'there', 'that', 'he', 'had', 'never', 'felt', 'before', 'Oh', 'God', 'he', 'thought', 'what', 'a', 'strenuous', 'career', 'it', 'is', 'that', \"I've\", 'chosen', 'Travelling', 'day', 'in', 'and', 'day', 'out', 'Doing', 'business', 'like', 'this', 'takes', 'much', 'more', 'effort', 'than', 'doing', 'your', 'own', 'business', 'at', 'home', 'and', 'on', 'top', 'of', 'that', \"there's\", 'the', 'curse', 'of', 'travelling', 'worries', 'about', 'making', 'train', 'connections', 'bad', 'and', 'irregular', 'food', 'contact', 'with', 'different', 'people', 'all', 'the', 'time', 'so', 'that', 'you', 'can', 'never', 'get', 'to', 'know', 'anyone', 'or', 'become', 'friendly', 'with', 'them', 'It', 'can', 'all', 'go', 'to', 'Hell', 'He', 'felt', 'a', 'slight', 'itch', 'up', 'on', 'his', 'belly', 'pushed', 'himself', 'slowly', 'up', 'on', 'his', 'back', 'towards', 'the', 'headboard', 'so', 'that', 'he', 'could', 'lift', 'his', 'head', 'better', 'found', 'where', 'the', 'itch', 'was', 'and', 'saw', 'that', 'it', 'was', 'covered', 'with', 'lots', 'of', 'little', 'white', 'spots', 'which', 'he', \"didn't\", 'know', 'what', 'to', 'make', 'of', 'and', 'when', 'he', 'tried', 'to', 'feel', 'the', 'place', 'with', 'one', 'of', 'his', 'legs', 'he', 'drew', 'it', 'quickly', 'back', 'because', 'as', 'soon', 'as', 'he', 'touched', 'it', 'he', 'was', 'overcome', 'by', 'a', 'cold', 'shudder', 'He', 'slid', 'back', 'into', 'his', 'former', 'position', 'Getting', 'up', 'early', 'all', 'the', 'time', 'he', 'thought', 'it', 'makes', 'you', 'stupid', \"You've\", 'got', 'to', 'get', 'enough', 'sleep', 'Other', 'travelling', 'salesmen', 'live', 'a', 'life', 'of', 'luxury', 'For', 'instance', 'whenever', 'I', 'go', 'back', 'to', 'the', 'guest', 'house', 'during', 'the', 'morning', 'to', 'copy', 'out', 'the', 'contract', 'these', 'gentlemen', 'are', 'always', 'still', 'sitting', 'there', 'eating', 'their', 'breakfasts', 'I', 'ought', 'to', 'just', 'try', 'that', 'with', 'my', 'boss', \"I'd\", 'get', 'kicked', 'out', 'on', 'the', 'spot', 'But', 'who', 'knows', 'maybe', 'that', 'would', 'be', 'the', 'best', 'thing', 'for', 'me', 'If', 'I', \"didn't\", 'have', 'my', 'parents', 'to', 'think', 'about', \"I'd\", 'have', 'given', 'in', 'my', 'notice', 'a', 'long', 'time', 'ago', \"I'd\", 'have', 'gone', 'up', 'to', 'the', 'boss', 'and', 'told', 'him', 'just', 'what', 'I', 'think', 'tell', 'him', 'everything', 'I', 'would', 'let', 'him', 'know', 'just', 'what', 'I', 'feel', \"He'd\", 'fall', 'right', 'off', 'his', 'desk', 'And', \"it's\", 'a', 'funny', 'sort', 'of', 'business', 'to', 'be', 'sitting', 'up', 'there', 'at', 'your', 'desk', 'talking', 'down', 'at', 'your', 'subordinates', 'from', 'up', 'there', 'especially', 'when', 'you', 'have', 'to', 'go', 'right', 'up', 'close', 'because', 'the', 'boss', 'is', 'hard', 'of', 'hearing', 'Well', \"there's\", 'still', 'some', 'hope', 'once', \"I've\", 'got', 'the', 'money', 'together', 'to', 'pay', 'off', 'my', 'parents', 'debt', 'to', 'him', 'another', 'five', 'or', 'six', 'years', 'I', 'suppose', \"that's\", 'definitely', 'what', \"I'll\", 'do', \"That's\", 'when', \"I'll\", 'make', 'the', 'big', 'change', 'First', 'of', 'all', 'though', \"I've\", 'got', 'to', 'get', 'up', 'my', 'train', 'leaves', 'at', 'five', 'And', 'he', 'looked', 'over', 'at', 'the', 'alarm', 'clock', 'ticking', 'on', 'the', 'chest', 'of', 'drawers', 'God', 'in', 'Heaven', 'he', 'thought', 'It', 'was', 'half', 'past', 'six', 'and', 'the', 'hands', 'were', 'quietly', 'moving', 'forwards', 'it', 'was', 'even', 'later', 'than', 'half', 'past', 'more', 'like', 'quarter', 'to', 'seven', 'Had', 'the', 'alarm', 'clock', 'not', 'rung', 'He', 'could', 'see', 'from', 'the', 'bed', 'that', 'it', 'had', 'been', 'set', 'for', 'four', \"o'clock\", 'as', 'it', 'should', 'have', 'been', 'it', 'certainly', 'must', 'have', 'rung', 'Yes', 'but', 'was', 'it', 'possible', 'to', 'quietly', 'sleep', 'through', 'that', 'furniture', 'rattling', 'noise', 'True', 'he', 'had', 'not', 'slept', 'peacefully', 'but', 'probably', 'all', 'the', 'more', 'deeply', 'because', 'of', 'that', 'What', 'should', 'he', 'do', 'now', 'The', 'next', 'train', 'went', 'at', 'seven', 'if', 'he', 'were', 'to', 'catch', 'that', 'he', 'would', 'have', 'to', 'rush', 'like', 'mad', 'and', 'the', 'collection', 'of', 'samples', 'was', 'still', 'not', 'packed', 'and', 'he', 'did', 'not', 'at', 'all', 'feel', 'particularly', 'fresh', 'and', 'lively', 'And', 'even', 'if', 'he', 'did', 'catch', 'the', 'train', 'he', 'would', 'not', 'avoid', 'his', \"boss's\", 'anger', 'as', 'the', 'office', 'assistant', 'would', 'have', 'been', 'there', 'to', 'see', 'the', 'five', \"o'clock\", 'train', 'go', 'he', 'would', 'have', 'put', 'in', 'his', 'report', 'about', \"Gregor's\", 'not', 'being', 'there', 'a', 'long', 'time', 'ago', 'The', 'office', 'assistant', 'was', 'the', \"boss's\", 'man', 'spineless', 'and', 'with', 'no', 'understanding', 'What', 'about', 'if', 'he', 'reported', 'sick', 'But', 'that', 'would', 'be', 'extremely', 'strained', 'and', 'suspicious', 'as', 'in', 'fifteen', 'years', 'of', 'service', 'Gregor', 'had', 'never', 'once', 'yet', 'been', 'ill', 'His', 'boss', 'would', 'certainly', 'come', 'round', 'with', 'the', 'doctor', 'from', 'the', 'medical', 'insurance', 'company', 'accuse', 'his', 'parents', 'of', 'having', 'a', 'lazy', 'son', 'and', 'accept', 'the', \"doctor's\", 'recommendation', 'not', 'to', 'make', 'any', 'claim', 'as', 'the', 'doctor', 'believed', 'that', 'no', 'one', 'was', 'ever', 'ill', 'but', 'that', 'many', 'were', 'workshy', 'And', \"what's\", 'more', 'would', 'he', 'have', 'been', 'entirely', 'wrong', 'in', 'this', 'case', 'Gregor', 'did', 'in', 'fact', 'apart', 'from', 'excessive', 'sleepiness', 'after', 'sleeping', 'for', 'so', 'long', 'feel', 'completely', 'well', 'and', 'even', 'felt', 'much', 'hungrier', 'than', 'usual', 'He', 'was', 'still', 'hurriedly', 'thinking', 'all', 'this', 'through', 'unable', 'to', 'decide', 'to', 'get', 'out', 'of', 'the', 'bed', 'when', 'the', 'clock', 'struck', 'quarter', 'to', 'seven', 'There', 'was', 'a', 'cautious', 'knock', 'at', 'the', 'door', 'near', 'his', 'head', 'Gregor', 'somebody', 'called', 'it', 'was', 'his', 'mother', \"it's\", 'quarter', 'to', 'seven', \"Didn't\", 'you', 'want', 'to', 'go', 'somewhere', 'That', 'gentle', 'voice', 'Gregor', 'was', 'shocked', 'when', 'he', 'heard', 'his', 'own']\n"
     ]
    }
   ],
   "source": [
    "metamorph_tokens_raw = nltk.regexp_tokenize(metamorph, pattern)\n",
    "print(metamorph_tokens_raw[:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25195"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## length of metamorph_tokens_raw\n",
    "\n",
    "len(metamorph_tokens_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 63,\n",
       "         'Project': 84,\n",
       "         'Gutenberg': 84,\n",
       "         'EBook': 3,\n",
       "         'of': 541,\n",
       "         'Metamorphosis': 4,\n",
       "         'by': 76,\n",
       "         'Franz': 4,\n",
       "         'Kafka': 4,\n",
       "         'Translated': 3,\n",
       "         'David': 5,\n",
       "         'Wyllie': 5,\n",
       "         'This': 20,\n",
       "         'eBook': 7,\n",
       "         'is': 53,\n",
       "         'for': 180,\n",
       "         'the': 1265,\n",
       "         'use': 20,\n",
       "         'anyone': 13,\n",
       "         'anywhere': 4,\n",
       "         'at': 174,\n",
       "         'no': 75,\n",
       "         'cost': 3,\n",
       "         'and': 680,\n",
       "         'with': 241,\n",
       "         'almost': 20,\n",
       "         'restrictions': 2,\n",
       "         'whatsoever': 3,\n",
       "         'You': 16,\n",
       "         'may': 15,\n",
       "         'copy': 13,\n",
       "         'it': 329,\n",
       "         'give': 14,\n",
       "         'away': 35,\n",
       "         'or': 113,\n",
       "         're': 3,\n",
       "         'under': 23,\n",
       "         'terms': 20,\n",
       "         'License': 10,\n",
       "         'included': 4,\n",
       "         'this': 120,\n",
       "         'online': 4,\n",
       "         'www': 6,\n",
       "         'gutenberg': 6,\n",
       "         'net': 6,\n",
       "         'a': 341,\n",
       "         'COPYRIGHTED': 1,\n",
       "         'Details': 1,\n",
       "         'Below': 1,\n",
       "         'Please': 5,\n",
       "         'follow': 2,\n",
       "         'copyright': 17,\n",
       "         'guidelines': 1,\n",
       "         'in': 395,\n",
       "         'file': 3,\n",
       "         'Title': 1,\n",
       "         'Author': 1,\n",
       "         'Translator': 1,\n",
       "         'Release': 1,\n",
       "         'Date': 1,\n",
       "         'August': 1,\n",
       "         'First': 3,\n",
       "         'posted': 6,\n",
       "         'May': 2,\n",
       "         'Last': 1,\n",
       "         'updated': 1,\n",
       "         'Language': 1,\n",
       "         'English': 1,\n",
       "         'START': 2,\n",
       "         'OF': 10,\n",
       "         'THIS': 5,\n",
       "         'PROJECT': 4,\n",
       "         'GUTENBERG': 4,\n",
       "         'EBOOK': 2,\n",
       "         'METAMORPHOSIS': 2,\n",
       "         'Copyright': 2,\n",
       "         'C': 3,\n",
       "         'I': 58,\n",
       "         'One': 7,\n",
       "         'morning': 22,\n",
       "         'when': 62,\n",
       "         'Gregor': 199,\n",
       "         'Samsa': 34,\n",
       "         'woke': 2,\n",
       "         'from': 131,\n",
       "         'troubled': 1,\n",
       "         'dreams': 2,\n",
       "         'he': 498,\n",
       "         'found': 16,\n",
       "         'himself': 87,\n",
       "         'transformed': 1,\n",
       "         'his': 524,\n",
       "         'bed': 29,\n",
       "         'into': 78,\n",
       "         'horrible': 3,\n",
       "         'vermin': 1,\n",
       "         'He': 82,\n",
       "         'lay': 19,\n",
       "         'on': 160,\n",
       "         'armour': 1,\n",
       "         'like': 38,\n",
       "         'back': 82,\n",
       "         'if': 92,\n",
       "         'lifted': 6,\n",
       "         'head': 38,\n",
       "         'little': 49,\n",
       "         'could': 120,\n",
       "         'see': 40,\n",
       "         'brown': 4,\n",
       "         'belly': 3,\n",
       "         'slightly': 8,\n",
       "         'domed': 1,\n",
       "         'divided': 1,\n",
       "         'arches': 1,\n",
       "         'stiff': 3,\n",
       "         'sections': 1,\n",
       "         'bedding': 1,\n",
       "         'was': 407,\n",
       "         'hardly': 13,\n",
       "         'able': 23,\n",
       "         'to': 827,\n",
       "         'cover': 1,\n",
       "         'seemed': 26,\n",
       "         'ready': 4,\n",
       "         'slide': 1,\n",
       "         'off': 19,\n",
       "         'any': 80,\n",
       "         'moment': 10,\n",
       "         'His': 26,\n",
       "         'many': 6,\n",
       "         'legs': 20,\n",
       "         'pitifully': 1,\n",
       "         'thin': 2,\n",
       "         'compared': 1,\n",
       "         'size': 2,\n",
       "         'rest': 7,\n",
       "         'him': 188,\n",
       "         'waved': 1,\n",
       "         'about': 72,\n",
       "         'helplessly': 1,\n",
       "         'as': 243,\n",
       "         'looked': 29,\n",
       "         \"What's\": 5,\n",
       "         'happened': 15,\n",
       "         'me': 17,\n",
       "         'thought': 32,\n",
       "         'It': 38,\n",
       "         \"wasn't\": 5,\n",
       "         'dream': 2,\n",
       "         'room': 131,\n",
       "         'proper': 3,\n",
       "         'human': 5,\n",
       "         'although': 17,\n",
       "         'too': 30,\n",
       "         'small': 6,\n",
       "         'peacefully': 3,\n",
       "         'between': 7,\n",
       "         'its': 22,\n",
       "         'four': 4,\n",
       "         'familiar': 2,\n",
       "         'walls': 7,\n",
       "         'A': 3,\n",
       "         'collection': 6,\n",
       "         'textile': 1,\n",
       "         'samples': 3,\n",
       "         'spread': 8,\n",
       "         'out': 114,\n",
       "         'table': 19,\n",
       "         'travelling': 7,\n",
       "         'salesman': 2,\n",
       "         'above': 4,\n",
       "         'there': 95,\n",
       "         'hung': 4,\n",
       "         'picture': 6,\n",
       "         'that': 337,\n",
       "         'had': 350,\n",
       "         'recently': 3,\n",
       "         'cut': 5,\n",
       "         'an': 34,\n",
       "         'illustrated': 1,\n",
       "         'magazine': 1,\n",
       "         'housed': 1,\n",
       "         'nice': 7,\n",
       "         'gilded': 1,\n",
       "         'frame': 2,\n",
       "         'showed': 3,\n",
       "         'lady': 4,\n",
       "         'fitted': 2,\n",
       "         'fur': 4,\n",
       "         'hat': 4,\n",
       "         'boa': 1,\n",
       "         'who': 35,\n",
       "         'sat': 14,\n",
       "         'upright': 9,\n",
       "         'raising': 4,\n",
       "         'heavy': 4,\n",
       "         'muff': 1,\n",
       "         'covered': 9,\n",
       "         'whole': 18,\n",
       "         'her': 183,\n",
       "         'lower': 7,\n",
       "         'arm': 4,\n",
       "         'towards': 12,\n",
       "         'viewer': 1,\n",
       "         'then': 70,\n",
       "         'turned': 19,\n",
       "         'look': 22,\n",
       "         'window': 19,\n",
       "         'dull': 2,\n",
       "         'weather': 2,\n",
       "         'Drops': 1,\n",
       "         'rain': 3,\n",
       "         'be': 136,\n",
       "         'heard': 19,\n",
       "         'hitting': 2,\n",
       "         'pane': 2,\n",
       "         'which': 41,\n",
       "         'made': 40,\n",
       "         'feel': 19,\n",
       "         'quite': 30,\n",
       "         'sad': 3,\n",
       "         'How': 3,\n",
       "         'sleep': 14,\n",
       "         'bit': 4,\n",
       "         'longer': 18,\n",
       "         'forget': 6,\n",
       "         'all': 148,\n",
       "         'nonsense': 1,\n",
       "         'but': 143,\n",
       "         'something': 27,\n",
       "         'unable': 6,\n",
       "         'do': 57,\n",
       "         'because': 22,\n",
       "         'used': 23,\n",
       "         'sleeping': 6,\n",
       "         'right': 21,\n",
       "         'present': 9,\n",
       "         'state': 15,\n",
       "         \"couldn't\": 8,\n",
       "         'get': 44,\n",
       "         'position': 8,\n",
       "         'However': 3,\n",
       "         'hard': 12,\n",
       "         'threw': 5,\n",
       "         'onto': 21,\n",
       "         'always': 18,\n",
       "         'rolled': 3,\n",
       "         'where': 39,\n",
       "         'must': 28,\n",
       "         'have': 111,\n",
       "         'tried': 12,\n",
       "         'hundred': 1,\n",
       "         'times': 15,\n",
       "         'shut': 9,\n",
       "         'eyes': 19,\n",
       "         'so': 84,\n",
       "         \"wouldn't\": 5,\n",
       "         'floundering': 1,\n",
       "         'only': 60,\n",
       "         'stopped': 9,\n",
       "         'began': 18,\n",
       "         'mild': 1,\n",
       "         'pain': 11,\n",
       "         'never': 20,\n",
       "         'felt': 15,\n",
       "         'before': 37,\n",
       "         'Oh': 5,\n",
       "         'God': 6,\n",
       "         'what': 57,\n",
       "         'strenuous': 1,\n",
       "         'career': 1,\n",
       "         \"I've\": 4,\n",
       "         'chosen': 2,\n",
       "         'Travelling': 1,\n",
       "         'day': 28,\n",
       "         'Doing': 1,\n",
       "         'business': 17,\n",
       "         'takes': 2,\n",
       "         'much': 39,\n",
       "         'more': 72,\n",
       "         'effort': 12,\n",
       "         'than': 57,\n",
       "         'doing': 13,\n",
       "         'your': 24,\n",
       "         'own': 10,\n",
       "         'home': 19,\n",
       "         'top': 3,\n",
       "         \"there's\": 3,\n",
       "         'curse': 2,\n",
       "         'worries': 2,\n",
       "         'making': 11,\n",
       "         'train': 9,\n",
       "         'connections': 1,\n",
       "         'bad': 3,\n",
       "         'irregular': 1,\n",
       "         'food': 14,\n",
       "         'contact': 4,\n",
       "         'different': 8,\n",
       "         'people': 9,\n",
       "         'time': 59,\n",
       "         'you': 99,\n",
       "         'can': 24,\n",
       "         'know': 18,\n",
       "         'become': 19,\n",
       "         'friendly': 5,\n",
       "         'them': 64,\n",
       "         'go': 40,\n",
       "         'Hell': 1,\n",
       "         'slight': 4,\n",
       "         'itch': 2,\n",
       "         'up': 84,\n",
       "         'pushed': 12,\n",
       "         'slowly': 22,\n",
       "         'headboard': 1,\n",
       "         'lift': 3,\n",
       "         'better': 20,\n",
       "         'saw': 12,\n",
       "         'lots': 1,\n",
       "         'white': 7,\n",
       "         'spots': 1,\n",
       "         \"didn't\": 10,\n",
       "         'make': 26,\n",
       "         'place': 9,\n",
       "         'one': 68,\n",
       "         'drew': 4,\n",
       "         'quickly': 11,\n",
       "         'soon': 30,\n",
       "         'touched': 4,\n",
       "         'overcome': 5,\n",
       "         'cold': 3,\n",
       "         'shudder': 1,\n",
       "         'slid': 3,\n",
       "         'former': 1,\n",
       "         'Getting': 1,\n",
       "         'early': 6,\n",
       "         'makes': 3,\n",
       "         'stupid': 2,\n",
       "         \"You've\": 2,\n",
       "         'got': 21,\n",
       "         'enough': 20,\n",
       "         'Other': 2,\n",
       "         'salesmen': 3,\n",
       "         'live': 4,\n",
       "         'life': 14,\n",
       "         'luxury': 1,\n",
       "         'For': 10,\n",
       "         'instance': 2,\n",
       "         'whenever': 6,\n",
       "         'guest': 1,\n",
       "         'house': 2,\n",
       "         'during': 5,\n",
       "         'contract': 1,\n",
       "         'these': 15,\n",
       "         'gentlemen': 24,\n",
       "         'are': 26,\n",
       "         'still': 44,\n",
       "         'sitting': 3,\n",
       "         'eating': 8,\n",
       "         'their': 68,\n",
       "         'breakfasts': 2,\n",
       "         'ought': 4,\n",
       "         'just': 46,\n",
       "         'try': 6,\n",
       "         'my': 20,\n",
       "         'boss': 9,\n",
       "         \"I'd\": 5,\n",
       "         'kicked': 1,\n",
       "         'spot': 3,\n",
       "         'But': 32,\n",
       "         'knows': 1,\n",
       "         'maybe': 4,\n",
       "         'would': 184,\n",
       "         'best': 4,\n",
       "         'thing': 11,\n",
       "         'If': 30,\n",
       "         'parents': 26,\n",
       "         'think': 20,\n",
       "         'given': 7,\n",
       "         'notice': 10,\n",
       "         'long': 22,\n",
       "         'ago': 3,\n",
       "         'gone': 5,\n",
       "         'told': 9,\n",
       "         'tell': 6,\n",
       "         'everything': 28,\n",
       "         'let': 26,\n",
       "         \"He'd\": 1,\n",
       "         'fall': 8,\n",
       "         'desk': 9,\n",
       "         'And': 30,\n",
       "         \"it's\": 12,\n",
       "         'funny': 1,\n",
       "         'sort': 6,\n",
       "         'talking': 4,\n",
       "         'down': 35,\n",
       "         'subordinates': 1,\n",
       "         'especially': 13,\n",
       "         'close': 12,\n",
       "         'hearing': 2,\n",
       "         'Well': 5,\n",
       "         'some': 39,\n",
       "         'hope': 5,\n",
       "         'once': 15,\n",
       "         'money': 16,\n",
       "         'together': 9,\n",
       "         'pay': 6,\n",
       "         'debt': 3,\n",
       "         'another': 8,\n",
       "         'five': 6,\n",
       "         'six': 3,\n",
       "         'years': 8,\n",
       "         'suppose': 1,\n",
       "         \"that's\": 6,\n",
       "         'definitely': 2,\n",
       "         \"I'll\": 8,\n",
       "         \"That's\": 5,\n",
       "         'big': 3,\n",
       "         'change': 5,\n",
       "         'though': 30,\n",
       "         'leaves': 1,\n",
       "         'over': 32,\n",
       "         'alarm': 6,\n",
       "         'clock': 5,\n",
       "         'ticking': 1,\n",
       "         'chest': 14,\n",
       "         'drawers': 9,\n",
       "         'Heaven': 1,\n",
       "         'half': 8,\n",
       "         'past': 6,\n",
       "         'hands': 19,\n",
       "         'were': 66,\n",
       "         'quietly': 5,\n",
       "         'moving': 11,\n",
       "         'forwards': 4,\n",
       "         'even': 79,\n",
       "         'later': 11,\n",
       "         'quarter': 6,\n",
       "         'seven': 8,\n",
       "         'Had': 2,\n",
       "         'not': 195,\n",
       "         'rung': 2,\n",
       "         'been': 101,\n",
       "         'set': 20,\n",
       "         \"o'clock\": 7,\n",
       "         'should': 18,\n",
       "         'certainly': 13,\n",
       "         'Yes': 5,\n",
       "         'possible': 18,\n",
       "         'through': 21,\n",
       "         'furniture': 15,\n",
       "         'rattling': 1,\n",
       "         'noise': 8,\n",
       "         'True': 1,\n",
       "         'slept': 2,\n",
       "         'probably': 15,\n",
       "         'deeply': 3,\n",
       "         'What': 10,\n",
       "         'now': 76,\n",
       "         'next': 18,\n",
       "         'went': 32,\n",
       "         'catch': 4,\n",
       "         'rush': 5,\n",
       "         'mad': 2,\n",
       "         'packed': 1,\n",
       "         'did': 71,\n",
       "         'particularly': 3,\n",
       "         'fresh': 6,\n",
       "         'lively': 3,\n",
       "         'avoid': 3,\n",
       "         \"boss's\": 2,\n",
       "         'anger': 3,\n",
       "         'office': 8,\n",
       "         'assistant': 2,\n",
       "         'put': 14,\n",
       "         'report': 3,\n",
       "         \"Gregor's\": 99,\n",
       "         'being': 26,\n",
       "         'man': 7,\n",
       "         'spineless': 1,\n",
       "         'understanding': 3,\n",
       "         'reported': 2,\n",
       "         'sick': 1,\n",
       "         'extremely': 3,\n",
       "         'strained': 1,\n",
       "         'suspicious': 3,\n",
       "         'fifteen': 2,\n",
       "         'service': 2,\n",
       "         'yet': 2,\n",
       "         'ill': 7,\n",
       "         'come': 29,\n",
       "         'round': 25,\n",
       "         'doctor': 6,\n",
       "         'medical': 1,\n",
       "         'insurance': 1,\n",
       "         'company': 3,\n",
       "         'accuse': 2,\n",
       "         'having': 8,\n",
       "         'lazy': 1,\n",
       "         'son': 2,\n",
       "         'accept': 2,\n",
       "         \"doctor's\": 1,\n",
       "         'recommendation': 1,\n",
       "         'claim': 2,\n",
       "         'believed': 1,\n",
       "         'ever': 12,\n",
       "         'workshy': 1,\n",
       "         \"what's\": 5,\n",
       "         'entirely': 4,\n",
       "         'wrong': 6,\n",
       "         'case': 1,\n",
       "         'fact': 5,\n",
       "         'apart': 3,\n",
       "         'excessive': 1,\n",
       "         'sleepiness': 1,\n",
       "         'after': 34,\n",
       "         'completely': 9,\n",
       "         'well': 27,\n",
       "         'hungrier': 2,\n",
       "         'usual': 5,\n",
       "         'hurriedly': 7,\n",
       "         'thinking': 1,\n",
       "         'decide': 1,\n",
       "         'struck': 6,\n",
       "         'There': 9,\n",
       "         'cautious': 2,\n",
       "         'knock': 3,\n",
       "         'door': 87,\n",
       "         'near': 7,\n",
       "         'somebody': 4,\n",
       "         'called': 14,\n",
       "         'mother': 79,\n",
       "         \"Didn't\": 1,\n",
       "         'want': 11,\n",
       "         'somewhere': 4,\n",
       "         'That': 9,\n",
       "         'gentle': 1,\n",
       "         'voice': 17,\n",
       "         'shocked': 5,\n",
       "         'answering': 1,\n",
       "         'recognised': 1,\n",
       "         'As': 9,\n",
       "         'deep': 3,\n",
       "         'inside': 3,\n",
       "         'painful': 2,\n",
       "         'uncontrollable': 1,\n",
       "         'squeaking': 1,\n",
       "         'mixed': 3,\n",
       "         'words': 12,\n",
       "         'first': 42,\n",
       "         'echo': 1,\n",
       "         'unclear': 1,\n",
       "         'leaving': 6,\n",
       "         'hearer': 1,\n",
       "         'unsure': 1,\n",
       "         'whether': 10,\n",
       "         'properly': 3,\n",
       "         'wanted': 28,\n",
       "         'full': 17,\n",
       "         'answer': 4,\n",
       "         'explain': 2,\n",
       "         'circumstances': 2,\n",
       "         'contented': 1,\n",
       "         'saying': 4,\n",
       "         'yes': 3,\n",
       "         'thank': 2,\n",
       "         \"I'm\": 15,\n",
       "         'getting': 6,\n",
       "         'noticed': 9,\n",
       "         'outside': 8,\n",
       "         'wooden': 2,\n",
       "         'satisfied': 1,\n",
       "         'explanation': 5,\n",
       "         'shuffled': 1,\n",
       "         'short': 5,\n",
       "         'conversation': 7,\n",
       "         'other': 55,\n",
       "         'members': 2,\n",
       "         'family': 31,\n",
       "         'aware': 6,\n",
       "         'against': 28,\n",
       "         'expectations': 1,\n",
       "         'father': 93,\n",
       "         'came': 31,\n",
       "         'knocking': 3,\n",
       "         'side': 22,\n",
       "         'doors': 10,\n",
       "         'gently': 4,\n",
       "         'fist': 2,\n",
       "         'while': 40,\n",
       "         'again': 39,\n",
       "         'warning': 1,\n",
       "         'deepness': 1,\n",
       "         'At': 11,\n",
       "         'sister': 96,\n",
       "         'plaintively': 1,\n",
       "         \"Aren't\": 2,\n",
       "         'Do': 4,\n",
       "         'need': 8,\n",
       "         'anything': 15,\n",
       "         'answered': 4,\n",
       "         'both': 9,\n",
       "         'sides': 5,\n",
       "         'remove': 5,\n",
       "         'strangeness': 1,\n",
       "         'enunciating': 1,\n",
       "         'very': 34,\n",
       "         'carefully': 9,\n",
       "         'putting': 2,\n",
       "         'pauses': 1,\n",
       "         'each': 18,\n",
       "         'individual': 7,\n",
       "         'word': 9,\n",
       "         'breakfast': 7,\n",
       "         'whispered': 2,\n",
       "         'open': 35,\n",
       "         'beg': 2,\n",
       "         'however': 8,\n",
       "         'opening': 3,\n",
       "         'instead': 6,\n",
       "         'congratulated': 1,\n",
       "         'habit': 6,\n",
       "         'acquired': 2,\n",
       "         'locking': 1,\n",
       "         'night': 10,\n",
       "         'peace': 8,\n",
       "         'without': 47,\n",
       "         'disturbed': 4,\n",
       "         'dressed': 9,\n",
       "         'most': 13,\n",
       "         'Only': 3,\n",
       "         'consider': 3,\n",
       "         'bring': 8,\n",
       "         'thoughts': 4,\n",
       "         'sensible': 4,\n",
       "         'conclusions': 2,\n",
       "         'lying': 6,\n",
       "         'remembered': 3,\n",
       "         'often': 16,\n",
       "         'perhaps': 15,\n",
       "         'caused': 6,\n",
       "         'awkwardly': 1,\n",
       "         'pure': 1,\n",
       "         'imagination': 1,\n",
       "         'wondered': 1,\n",
       "         'how': 30,\n",
       "         'imaginings': 1,\n",
       "         'resolve': 1,\n",
       "         'themselves': 10,\n",
       "         'today': 4,\n",
       "         'slightest': 6,\n",
       "         'doubt': 1,\n",
       "         'nothing': 16,\n",
       "         'sign': 8,\n",
       "         'serious': 7,\n",
       "         'occupational': 1,\n",
       "         'hazard': 1,\n",
       "         'simple': 3,\n",
       "         'matter': 2,\n",
       "         'throw': 5,\n",
       "         'covers': 2,\n",
       "         'blow': 2,\n",
       "         'they': 137,\n",
       "         'fell': 9,\n",
       "         'became': 15,\n",
       "         'difficult': 7,\n",
       "         'exceptionally': 1,\n",
       "         'broad': 3,\n",
       "         'arms': 13,\n",
       "         'push': 4,\n",
       "         'those': 4,\n",
       "         'continuously': 1,\n",
       "         'directions': 3,\n",
       "         'moreover': 2,\n",
       "         'control': 4,\n",
       "         'bend': 2,\n",
       "         'stretch': 3,\n",
       "         'itself': 4,\n",
       "         'finally': 10,\n",
       "         'managed': 3,\n",
       "         'leg': 1,\n",
       "         'others': 9,\n",
       "         'free': 11,\n",
       "         'move': 16,\n",
       "         'painfully': 6,\n",
       "         \"can't\": 8,\n",
       "         'done': 12,\n",
       "         'said': 51,\n",
       "         \"don't\": 12,\n",
       "         'keep': 13,\n",
       "         'trying': 4,\n",
       "         'part': 11,\n",
       "         'body': 24,\n",
       "         'seen': 13,\n",
       "         'imagine': 2,\n",
       "         'frenzy': 1,\n",
       "         'carelessly': 1,\n",
       "         'shoved': 1,\n",
       "         'force': 8,\n",
       "         'gather': 2,\n",
       "         'chose': 1,\n",
       "         'direction': 4,\n",
       "         'hit': 4,\n",
       "         'bedpost': 1,\n",
       "         'learned': 5,\n",
       "         'burning': 1,\n",
       "         'might': 17,\n",
       "         'sensitive': 2,\n",
       "         'So': 14,\n",
       "         'turning': 7,\n",
       "         'easily': 6,\n",
       "         'despite': 9,\n",
       "         'breadth': 1,\n",
       "         'weight': 3,\n",
       "         'bulk': 1,\n",
       "         'eventually': 1,\n",
       "         'followed': 5,\n",
       "         'last': 11,\n",
       "         'air': 6,\n",
       "         'occurred': 3,\n",
       "         'miracle': 2,\n",
       "         'injured': 3,\n",
       "         'afraid': 3,\n",
       "         'carry': 8,\n",
       "         'pushing': 4,\n",
       "         'forward': 12,\n",
       "         'same': 17,\n",
       "         'way': 64,\n",
       "         'price': 2,\n",
       "         'stay': 11,\n",
       "         'lose': 1,\n",
       "         'consciousness': 1,\n",
       "         'took': 18,\n",
       "         'earlier': 7,\n",
       "         'sighing': 1,\n",
       "         'watching': 3,\n",
       "         'struggled': 1,\n",
       "         'harder': 5,\n",
       "         'bringing': 3,\n",
       "         'order': 7,\n",
       "         'chaos': 1,\n",
       "         'whatever': 2,\n",
       "         'sacrifice': 2,\n",
       "         'remind': 2,\n",
       "         'calm': 7,\n",
       "         'consideration': 5,\n",
       "         'rushing': 1,\n",
       "         'desperate': 1,\n",
       "         'direct': 2,\n",
       "         'clearly': 10,\n",
       "         'unfortunately': 6,\n",
       "         'narrow': 3,\n",
       "         'street': 6,\n",
       "         'enveloped': 1,\n",
       "         'fog': 2,\n",
       "         'view': 4,\n",
       "         'confidence': 4,\n",
       "         'cheer': 1,\n",
       "         'offer': 1,\n",
       "         'Seven': 1,\n",
       "         'already': 24,\n",
       "         'breathing': 2,\n",
       "         'lightly': 3,\n",
       "         'expected': 7,\n",
       "         'total': 3,\n",
       "         'stillness': 1,\n",
       "         'things': 27,\n",
       "         'real': 2,\n",
       "         'natural': 1,\n",
       "         'Before': 2,\n",
       "         'strikes': 1,\n",
       "         'will': 14,\n",
       "         'work': 74,\n",
       "         'ask': 3,\n",
       "         'task': 2,\n",
       "         'swinging': 1,\n",
       "         'entire': 4,\n",
       "         'length': 1,\n",
       "         'succeeded': 1,\n",
       "         'falling': 3,\n",
       "         'kept': 8,\n",
       "         'raised': 5,\n",
       "         'injuring': 1,\n",
       "         'happen': 5,\n",
       "         'carpet': 6,\n",
       "         'main': 3,\n",
       "         'concern': 4,\n",
       "         'loud': 8,\n",
       "         'bound': 3,\n",
       "         'raise': 3,\n",
       "         'risked': 1,\n",
       "         'When': 7,\n",
       "         'sticking': 1,\n",
       "         'new': 15,\n",
       "         'method': 2,\n",
       "         'game': 1,\n",
       "         'rock': 1,\n",
       "         'forth': 12,\n",
       "         'help': 17,\n",
       "         'Two': 1,\n",
       "         'strong': 4,\n",
       "         'maid': 7,\n",
       "         'mind': 4,\n",
       "         'dome': 1,\n",
       "         'peel': 1,\n",
       "         'load': 2,\n",
       "         'patient': 5,\n",
       "         'careful': 4,\n",
       "         'swang': 2,\n",
       "         'floor': 22,\n",
       "         'hopefully': 1,\n",
       "         'find': 6,\n",
       "         'Should': 1,\n",
       "         'really': 23,\n",
       "         'call': 4,\n",
       "         'locked': 5,\n",
       "         'Despite': 2,\n",
       "         'difficulty': 4,\n",
       "         'suppress': 1,\n",
       "         'smile': 4,\n",
       "         'After': 4,\n",
       "         'moved': 11,\n",
       "         'far': 9,\n",
       "         'across': 7,\n",
       "         'balance': 1,\n",
       "         'rocked': 1,\n",
       "         'ten': 2,\n",
       "         'final': 1,\n",
       "         'decision': 4,\n",
       "         'Then': 14,\n",
       "         'ring': 1,\n",
       "         'flat': 20,\n",
       "         \"That'll\": 1,\n",
       "         'someone': 5,\n",
       "         'froze': 2,\n",
       "         'danced': 1,\n",
       "         'around': 20,\n",
       "         'remained': 11,\n",
       "         'quiet': 10,\n",
       "         \"They're\": 1,\n",
       "         'caught': 3,\n",
       "         'nonsensical': 1,\n",
       "         'course': 16,\n",
       "         \"maid's\": 1,\n",
       "         'firm': 3,\n",
       "         'steps': 5,\n",
       "         'opened': 16,\n",
       "         'needed': 8,\n",
       "         'hear': 14,\n",
       "         \"visitor's\": 1,\n",
       "         'greeting': 1,\n",
       "         'knew': 8,\n",
       "         'chief': 37,\n",
       "         'clerk': 34,\n",
       "         'Why': 2,\n",
       "         'condemned': 1,\n",
       "         'immediately': 16,\n",
       "         'highly': 4,\n",
       "         'shortcoming': 1,\n",
       "         'Were': 1,\n",
       "         'employees': 6,\n",
       "         'every': 12,\n",
       "         'louts': 1,\n",
       "         'faithful': 1,\n",
       "         'devoted': 1,\n",
       "         'pangs': 1,\n",
       "         'conscience': 1,\n",
       "         'spend': 3,\n",
       "         'least': 11,\n",
       "         'couple': 2,\n",
       "         'hours': 8,\n",
       "         'Was': 3,\n",
       "         'trainees': 1,\n",
       "         'enquiries': 2,\n",
       "         'assuming': 1,\n",
       "         'necessary': 5,\n",
       "         'show': 5,\n",
       "         'innocent': 2,\n",
       "         'trusted': 1,\n",
       "         'wisdom': 1,\n",
       "         'investigate': 1,\n",
       "         'upset': 3,\n",
       "         'thump': 1,\n",
       "         'softened': 1,\n",
       "         'also': 17,\n",
       "         'elastic': 1,\n",
       "         'sound': 7,\n",
       "         'muffled': 1,\n",
       "         'noticeable': 1,\n",
       "         'held': 14,\n",
       "         'annoyed': 4,\n",
       "         'rubbed': 2,\n",
       "         \"Something's\": 1,\n",
       "         'fallen': 4,\n",
       "         'left': 32,\n",
       "         'concede': 1,\n",
       "         'gruff': 1,\n",
       "         'reply': 2,\n",
       "         'question': 7,\n",
       "         \"clerk's\": 3,\n",
       "         'footsteps': 1,\n",
       "         'polished': 2,\n",
       "         'boots': 2,\n",
       "         'adjoining': 1,\n",
       "         'From': 3,\n",
       "         'here': 19,\n",
       "         'daring': 1,\n",
       "         'has': 6,\n",
       "         'wants': 3,\n",
       "         'why': 7,\n",
       "         'leave': 11,\n",
       "         'We': 6,\n",
       "         'say': 18,\n",
       "         'anyway': 5,\n",
       "         'speak': 8,\n",
       "         'personally': 1,\n",
       "         'please': 6,\n",
       "         'sure': 6,\n",
       "         \"he'll\": 3,\n",
       "         'good': 18,\n",
       "         'forgive': 1,\n",
       "         'untidiness': 1,\n",
       "         'Good': 1,\n",
       "         'Mr': 23,\n",
       "         \"isn't\": 4,\n",
       "         'continued': 2,\n",
       "         'believe': 2,\n",
       "         'else': 8,\n",
       "         'missed': 1,\n",
       "         'lad': 1,\n",
       "         'thinks': 1,\n",
       "         'nearly': 9,\n",
       "         'cross': 1,\n",
       "         'goes': 1,\n",
       "         'evenings': 4,\n",
       "         \"he's\": 6,\n",
       "         'town': 3,\n",
       "         'week': 2,\n",
       "         'stayed': 5,\n",
       "         'evening': 18,\n",
       "         'sits': 1,\n",
       "         'us': 12,\n",
       "         'kitchen': 11,\n",
       "         'reads': 1,\n",
       "         'paper': 3,\n",
       "         'studies': 1,\n",
       "         'timetables': 1,\n",
       "         'idea': 5,\n",
       "         'relaxation': 2,\n",
       "         'working': 4,\n",
       "         'fretsaw': 2,\n",
       "         \"He's\": 2,\n",
       "         'two': 26,\n",
       "         'three': 25,\n",
       "         \"you'll\": 2,\n",
       "         'amazed': 4,\n",
       "         'hanging': 2,\n",
       "         'opens': 1,\n",
       "         'Anyway': 1,\n",
       "         'glad': 5,\n",
       "         \"you're\": 5,\n",
       "         'we': 29,\n",
       "         'ourselves': 2,\n",
       "         'stubborn': 2,\n",
       "         'thoughtfully': 1,\n",
       "         'miss': 1,\n",
       "         'explaining': 3,\n",
       "         'Mrs': 10,\n",
       "         'hand': 22,\n",
       "         'commerce': 1,\n",
       "         'unwell': 2,\n",
       "         'fortunately': 2,\n",
       "         'simply': 7,\n",
       "         'considerations': 1,\n",
       "         'Can': 1,\n",
       "         'asked': 16,\n",
       "         'impatiently': 1,\n",
       "         'No': 7,\n",
       "         'In': 10,\n",
       "         'silence': 3,\n",
       "         'cry': 2,\n",
       "         'join': 1,\n",
       "         'She': 24,\n",
       "         'begun': 1,\n",
       "         'she': 172,\n",
       "         'crying': 3,\n",
       "         'danger': 2,\n",
       "         'losing': 1,\n",
       "         'job': 8,\n",
       "         'pursue': 1,\n",
       "         'demands': 1,\n",
       "         'worry': 4,\n",
       "         'intention': 3,\n",
       "         'abandoning': 2,\n",
       "         'condition': 8,\n",
       "         'seriously': 5,\n",
       "         'minor': 1,\n",
       "         'discourtesy': 1,\n",
       "         'suitable': 3,\n",
       "         'excuse': 2,\n",
       "         'sacked': 2,\n",
       "         ...})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(metamorph_tokens_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'below', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'of', 'this']\n"
     ]
    }
   ],
   "source": [
    "metamorph_tokens = [i.lower() for i in metamorph_tokens_raw]\n",
    "print(metamorph_tokens[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"'\"\n",
    "\n",
    "metamorph_tokens = [i.replace(\"'\", \"\").replace(\"-\", \"\") for i in metamorph_tokens]\n",
    "\n",
    "\n",
    "# replace \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's check the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') + ['copyright', 'project', 'ebook', 'gutenberg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'use', 'terms', 'license', 'included', 'online', 'www', 'net', 'copyrighted', 'details', 'please', 'follow', 'guidelines', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'metamorphosis', 'c', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armour', 'like', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully']\n"
     ]
    }
   ],
   "source": [
    "metamorph_tokens_stopped = [w for w in metamorph_tokens if not w in stop_words]\n",
    "print(metamorph_tokens_stopped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Porter Stemmer \n",
    "<img src=\"https://cdn.homebrewersassociation.org/wp-content/uploads/Baltic_Porter_Feature-600x800.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "example = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "singles = [stemmer.stem(e) for e in example]\n",
    "print(*singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Snowball Stemmer\n",
    "<img src=\"https://localtvwiti.files.wordpress.com/2018/08/gettyimages-936380496.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic\n",
      "danish\n",
      "dutch\n",
      "english\n",
      "finnish\n",
      "french\n",
      "german\n",
      "hungarian\n",
      "italian\n",
      "norwegian\n",
      "porter\n",
      "portuguese\n",
      "romanian\n",
      "russian\n",
      "spanish\n",
      "swedish\n"
     ]
    }
   ],
   "source": [
    "print(*SnowballStemmer.languages, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabl\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"probably\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter vs Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Snowball on Metamorphosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metamorphosis',\n",
       " 'franz',\n",
       " 'kafka',\n",
       " 'translated',\n",
       " 'david',\n",
       " 'wyllie',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'give',\n",
       " 'away',\n",
       " 'use',\n",
       " 'terms',\n",
       " 'license',\n",
       " 'included',\n",
       " 'online',\n",
       " 'www',\n",
       " 'net',\n",
       " 'copyrighted',\n",
       " 'details',\n",
       " 'please',\n",
       " 'follow',\n",
       " 'guidelines',\n",
       " 'file',\n",
       " 'title',\n",
       " 'metamorphosis',\n",
       " 'author',\n",
       " 'franz',\n",
       " 'kafka',\n",
       " 'translator',\n",
       " 'david',\n",
       " 'wyllie',\n",
       " 'release',\n",
       " 'date',\n",
       " 'august',\n",
       " 'first',\n",
       " 'posted',\n",
       " 'may',\n",
       " 'last',\n",
       " 'updated',\n",
       " 'may',\n",
       " 'language',\n",
       " 'english',\n",
       " 'start',\n",
       " 'metamorphosis',\n",
       " 'c',\n",
       " 'david',\n",
       " 'wyllie',\n",
       " 'metamorphosis',\n",
       " 'franz',\n",
       " 'kafka',\n",
       " 'translated',\n",
       " 'david',\n",
       " 'wyllie',\n",
       " 'one',\n",
       " 'morning',\n",
       " 'gregor',\n",
       " 'samsa',\n",
       " 'woke',\n",
       " 'troubled',\n",
       " 'dreams',\n",
       " 'found',\n",
       " 'transformed',\n",
       " 'bed',\n",
       " 'horrible',\n",
       " 'vermin',\n",
       " 'lay',\n",
       " 'armour',\n",
       " 'like',\n",
       " 'back',\n",
       " 'lifted',\n",
       " 'head',\n",
       " 'little',\n",
       " 'could',\n",
       " 'see',\n",
       " 'brown',\n",
       " 'belly',\n",
       " 'slightly',\n",
       " 'domed',\n",
       " 'divided',\n",
       " 'arches',\n",
       " 'stiff',\n",
       " 'sections',\n",
       " 'bedding',\n",
       " 'hardly',\n",
       " 'able',\n",
       " 'cover',\n",
       " 'seemed',\n",
       " 'ready',\n",
       " 'slide',\n",
       " 'moment',\n",
       " 'many',\n",
       " 'legs',\n",
       " 'pitifully',\n",
       " 'thin',\n",
       " 'compared',\n",
       " 'size',\n",
       " 'rest',\n",
       " 'waved',\n",
       " 'helplessly',\n",
       " 'looked',\n",
       " 'whats',\n",
       " 'happened',\n",
       " 'thought',\n",
       " 'wasnt',\n",
       " 'dream',\n",
       " 'room',\n",
       " 'proper',\n",
       " 'human',\n",
       " 'room',\n",
       " 'although',\n",
       " 'little',\n",
       " 'small',\n",
       " 'lay',\n",
       " 'peacefully',\n",
       " 'four',\n",
       " 'familiar',\n",
       " 'walls',\n",
       " 'collection',\n",
       " 'textile',\n",
       " 'samples',\n",
       " 'lay',\n",
       " 'spread',\n",
       " 'table',\n",
       " 'samsa',\n",
       " 'travelling',\n",
       " 'salesman',\n",
       " 'hung',\n",
       " 'picture',\n",
       " 'recently',\n",
       " 'cut',\n",
       " 'illustrated',\n",
       " 'magazine',\n",
       " 'housed',\n",
       " 'nice',\n",
       " 'gilded',\n",
       " 'frame',\n",
       " 'showed',\n",
       " 'lady',\n",
       " 'fitted',\n",
       " 'fur',\n",
       " 'hat',\n",
       " 'fur',\n",
       " 'boa',\n",
       " 'sat',\n",
       " 'upright',\n",
       " 'raising',\n",
       " 'heavy',\n",
       " 'fur',\n",
       " 'muff',\n",
       " 'covered',\n",
       " 'whole',\n",
       " 'lower',\n",
       " 'arm',\n",
       " 'towards',\n",
       " 'viewer',\n",
       " 'gregor',\n",
       " 'turned',\n",
       " 'look',\n",
       " 'window',\n",
       " 'dull',\n",
       " 'weather',\n",
       " 'drops',\n",
       " 'rain',\n",
       " 'could',\n",
       " 'heard',\n",
       " 'hitting',\n",
       " 'pane',\n",
       " 'made',\n",
       " 'feel',\n",
       " 'quite',\n",
       " 'sad',\n",
       " 'sleep',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'longer',\n",
       " 'forget',\n",
       " 'nonsense',\n",
       " 'thought',\n",
       " 'something',\n",
       " 'unable',\n",
       " 'used',\n",
       " 'sleeping',\n",
       " 'right',\n",
       " 'present',\n",
       " 'state',\n",
       " 'couldnt',\n",
       " 'get',\n",
       " 'position',\n",
       " 'however',\n",
       " 'hard',\n",
       " 'threw',\n",
       " 'onto',\n",
       " 'right',\n",
       " 'always',\n",
       " 'rolled',\n",
       " 'back',\n",
       " 'must',\n",
       " 'tried',\n",
       " 'hundred',\n",
       " 'times',\n",
       " 'shut',\n",
       " 'eyes',\n",
       " 'wouldnt',\n",
       " 'look',\n",
       " 'floundering',\n",
       " 'legs',\n",
       " 'stopped',\n",
       " 'began',\n",
       " 'feel',\n",
       " 'mild',\n",
       " 'dull',\n",
       " 'pain',\n",
       " 'never',\n",
       " 'felt',\n",
       " 'oh',\n",
       " 'god',\n",
       " 'thought',\n",
       " 'strenuous',\n",
       " 'career',\n",
       " 'ive',\n",
       " 'chosen',\n",
       " 'travelling',\n",
       " 'day',\n",
       " 'day',\n",
       " 'business',\n",
       " 'like',\n",
       " 'takes',\n",
       " 'much',\n",
       " 'effort',\n",
       " 'business',\n",
       " 'home',\n",
       " 'top',\n",
       " 'theres',\n",
       " 'curse',\n",
       " 'travelling',\n",
       " 'worries',\n",
       " 'making',\n",
       " 'train',\n",
       " 'connections',\n",
       " 'bad',\n",
       " 'irregular',\n",
       " 'food',\n",
       " 'contact',\n",
       " 'different',\n",
       " 'people',\n",
       " 'time',\n",
       " 'never',\n",
       " 'get',\n",
       " 'know',\n",
       " 'anyone',\n",
       " 'become',\n",
       " 'friendly',\n",
       " 'go',\n",
       " 'hell',\n",
       " 'felt',\n",
       " 'slight',\n",
       " 'itch',\n",
       " 'belly',\n",
       " 'pushed',\n",
       " 'slowly',\n",
       " 'back',\n",
       " 'towards',\n",
       " 'headboard',\n",
       " 'could',\n",
       " 'lift',\n",
       " 'head',\n",
       " 'better',\n",
       " 'found',\n",
       " 'itch',\n",
       " 'saw',\n",
       " 'covered',\n",
       " 'lots',\n",
       " 'little',\n",
       " 'white',\n",
       " 'spots',\n",
       " 'didnt',\n",
       " 'know',\n",
       " 'make',\n",
       " 'tried',\n",
       " 'feel',\n",
       " 'place',\n",
       " 'one',\n",
       " 'legs',\n",
       " 'drew',\n",
       " 'quickly',\n",
       " 'back',\n",
       " 'soon',\n",
       " 'touched',\n",
       " 'overcome',\n",
       " 'cold',\n",
       " 'shudder',\n",
       " 'slid',\n",
       " 'back',\n",
       " 'former',\n",
       " 'position',\n",
       " 'getting',\n",
       " 'early',\n",
       " 'time',\n",
       " 'thought',\n",
       " 'makes',\n",
       " 'stupid',\n",
       " 'youve',\n",
       " 'got',\n",
       " 'get',\n",
       " 'enough',\n",
       " 'sleep',\n",
       " 'travelling',\n",
       " 'salesmen',\n",
       " 'live',\n",
       " 'life',\n",
       " 'luxury',\n",
       " 'instance',\n",
       " 'whenever',\n",
       " 'go',\n",
       " 'back',\n",
       " 'guest',\n",
       " 'house',\n",
       " 'morning',\n",
       " 'copy',\n",
       " 'contract',\n",
       " 'gentlemen',\n",
       " 'always',\n",
       " 'still',\n",
       " 'sitting',\n",
       " 'eating',\n",
       " 'breakfasts',\n",
       " 'ought',\n",
       " 'try',\n",
       " 'boss',\n",
       " 'id',\n",
       " 'get',\n",
       " 'kicked',\n",
       " 'spot',\n",
       " 'knows',\n",
       " 'maybe',\n",
       " 'would',\n",
       " 'best',\n",
       " 'thing',\n",
       " 'didnt',\n",
       " 'parents',\n",
       " 'think',\n",
       " 'id',\n",
       " 'given',\n",
       " 'notice',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'id',\n",
       " 'gone',\n",
       " 'boss',\n",
       " 'told',\n",
       " 'think',\n",
       " 'tell',\n",
       " 'everything',\n",
       " 'would',\n",
       " 'let',\n",
       " 'know',\n",
       " 'feel',\n",
       " 'hed',\n",
       " 'fall',\n",
       " 'right',\n",
       " 'desk',\n",
       " 'funny',\n",
       " 'sort',\n",
       " 'business',\n",
       " 'sitting',\n",
       " 'desk',\n",
       " 'talking',\n",
       " 'subordinates',\n",
       " 'especially',\n",
       " 'go',\n",
       " 'right',\n",
       " 'close',\n",
       " 'boss',\n",
       " 'hard',\n",
       " 'hearing',\n",
       " 'well',\n",
       " 'theres',\n",
       " 'still',\n",
       " 'hope',\n",
       " 'ive',\n",
       " 'got',\n",
       " 'money',\n",
       " 'together',\n",
       " 'pay',\n",
       " 'parents',\n",
       " 'debt',\n",
       " 'another',\n",
       " 'five',\n",
       " 'six',\n",
       " 'years',\n",
       " 'suppose',\n",
       " 'thats',\n",
       " 'definitely',\n",
       " 'ill',\n",
       " 'thats',\n",
       " 'ill',\n",
       " 'make',\n",
       " 'big',\n",
       " 'change',\n",
       " 'first',\n",
       " 'though',\n",
       " 'ive',\n",
       " 'got',\n",
       " 'get',\n",
       " 'train',\n",
       " 'leaves',\n",
       " 'five',\n",
       " 'looked',\n",
       " 'alarm',\n",
       " 'clock',\n",
       " 'ticking',\n",
       " 'chest',\n",
       " 'drawers',\n",
       " 'god',\n",
       " 'heaven',\n",
       " 'thought',\n",
       " 'half',\n",
       " 'past',\n",
       " 'six',\n",
       " 'hands',\n",
       " 'quietly',\n",
       " 'moving',\n",
       " 'forwards',\n",
       " 'even',\n",
       " 'later',\n",
       " 'half',\n",
       " 'past',\n",
       " 'like',\n",
       " 'quarter',\n",
       " 'seven',\n",
       " 'alarm',\n",
       " 'clock',\n",
       " 'rung',\n",
       " 'could',\n",
       " 'see',\n",
       " 'bed',\n",
       " 'set',\n",
       " 'four',\n",
       " 'oclock',\n",
       " 'certainly',\n",
       " 'must',\n",
       " 'rung',\n",
       " 'yes',\n",
       " 'possible',\n",
       " 'quietly',\n",
       " 'sleep',\n",
       " 'furniture',\n",
       " 'rattling',\n",
       " 'noise',\n",
       " 'true',\n",
       " 'slept',\n",
       " 'peacefully',\n",
       " 'probably',\n",
       " 'deeply',\n",
       " 'next',\n",
       " 'train',\n",
       " 'went',\n",
       " 'seven',\n",
       " 'catch',\n",
       " 'would',\n",
       " 'rush',\n",
       " 'like',\n",
       " 'mad',\n",
       " 'collection',\n",
       " 'samples',\n",
       " 'still',\n",
       " 'packed',\n",
       " 'feel',\n",
       " 'particularly',\n",
       " 'fresh',\n",
       " 'lively',\n",
       " 'even',\n",
       " 'catch',\n",
       " 'train',\n",
       " 'would',\n",
       " 'avoid',\n",
       " 'bosss',\n",
       " 'anger',\n",
       " 'office',\n",
       " 'assistant',\n",
       " 'would',\n",
       " 'see',\n",
       " 'five',\n",
       " 'oclock',\n",
       " 'train',\n",
       " 'go',\n",
       " 'would',\n",
       " 'put',\n",
       " 'report',\n",
       " 'gregors',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'office',\n",
       " 'assistant',\n",
       " 'bosss',\n",
       " 'man',\n",
       " 'spineless',\n",
       " 'understanding',\n",
       " 'reported',\n",
       " 'sick',\n",
       " 'would',\n",
       " 'extremely',\n",
       " 'strained',\n",
       " 'suspicious',\n",
       " 'fifteen',\n",
       " 'years',\n",
       " 'service',\n",
       " 'gregor',\n",
       " 'never',\n",
       " 'yet',\n",
       " 'ill',\n",
       " 'boss',\n",
       " 'would',\n",
       " 'certainly',\n",
       " 'come',\n",
       " 'round',\n",
       " 'doctor',\n",
       " 'medical',\n",
       " 'insurance',\n",
       " 'company',\n",
       " 'accuse',\n",
       " 'parents',\n",
       " 'lazy',\n",
       " 'son',\n",
       " 'accept',\n",
       " 'doctors',\n",
       " 'recommendation',\n",
       " 'make',\n",
       " 'claim',\n",
       " 'doctor',\n",
       " 'believed',\n",
       " 'one',\n",
       " 'ever',\n",
       " 'ill',\n",
       " 'many',\n",
       " 'workshy',\n",
       " 'whats',\n",
       " 'would',\n",
       " 'entirely',\n",
       " 'wrong',\n",
       " 'case',\n",
       " 'gregor',\n",
       " 'fact',\n",
       " 'apart',\n",
       " 'excessive',\n",
       " 'sleepiness',\n",
       " 'sleeping',\n",
       " 'long',\n",
       " 'feel',\n",
       " 'completely',\n",
       " 'well',\n",
       " 'even',\n",
       " 'felt',\n",
       " 'much',\n",
       " 'hungrier',\n",
       " 'usual',\n",
       " 'still',\n",
       " 'hurriedly',\n",
       " 'thinking',\n",
       " 'unable',\n",
       " 'decide',\n",
       " 'get',\n",
       " 'bed',\n",
       " 'clock',\n",
       " 'struck',\n",
       " 'quarter',\n",
       " 'seven',\n",
       " 'cautious',\n",
       " 'knock',\n",
       " 'door',\n",
       " 'near',\n",
       " 'head',\n",
       " 'gregor',\n",
       " 'somebody',\n",
       " 'called',\n",
       " 'mother',\n",
       " 'quarter',\n",
       " 'seven',\n",
       " 'didnt',\n",
       " 'want',\n",
       " 'go',\n",
       " 'somewhere',\n",
       " 'gentle',\n",
       " 'voice',\n",
       " 'gregor',\n",
       " 'shocked',\n",
       " 'heard',\n",
       " 'voice',\n",
       " 'answering',\n",
       " 'could',\n",
       " 'hardly',\n",
       " 'recognised',\n",
       " 'voice',\n",
       " 'deep',\n",
       " 'inside',\n",
       " 'painful',\n",
       " 'uncontrollable',\n",
       " 'squeaking',\n",
       " 'mixed',\n",
       " 'words',\n",
       " 'could',\n",
       " 'made',\n",
       " 'first',\n",
       " 'sort',\n",
       " 'echo',\n",
       " 'made',\n",
       " 'unclear',\n",
       " 'leaving',\n",
       " 'hearer',\n",
       " 'unsure',\n",
       " 'whether',\n",
       " 'heard',\n",
       " 'properly',\n",
       " 'gregor',\n",
       " 'wanted',\n",
       " 'give',\n",
       " 'full',\n",
       " 'answer',\n",
       " 'explain',\n",
       " 'everything',\n",
       " 'circumstances',\n",
       " 'contented',\n",
       " 'saying',\n",
       " 'yes',\n",
       " 'mother',\n",
       " 'yes',\n",
       " 'thank',\n",
       " 'im',\n",
       " 'getting',\n",
       " 'change',\n",
       " 'gregors',\n",
       " 'voice',\n",
       " 'probably',\n",
       " 'could',\n",
       " 'noticed',\n",
       " 'outside',\n",
       " 'wooden',\n",
       " 'door',\n",
       " 'mother',\n",
       " 'satisfied',\n",
       " 'explanation',\n",
       " 'shuffled',\n",
       " 'away',\n",
       " 'short',\n",
       " 'conversation',\n",
       " 'made',\n",
       " 'members',\n",
       " 'family',\n",
       " 'aware',\n",
       " 'gregor',\n",
       " 'expectations',\n",
       " 'still',\n",
       " 'home',\n",
       " 'soon',\n",
       " 'father',\n",
       " 'came',\n",
       " 'knocking',\n",
       " 'one',\n",
       " 'side',\n",
       " 'doors',\n",
       " 'gently',\n",
       " 'fist',\n",
       " 'gregor',\n",
       " 'gregor',\n",
       " 'called',\n",
       " 'whats',\n",
       " 'wrong',\n",
       " 'short',\n",
       " 'called',\n",
       " 'warning',\n",
       " 'deepness',\n",
       " 'voice',\n",
       " 'gregor',\n",
       " 'gregor',\n",
       " 'side',\n",
       " 'door',\n",
       " 'sister',\n",
       " 'came',\n",
       " 'plaintively',\n",
       " 'gregor',\n",
       " 'arent',\n",
       " 'well',\n",
       " 'need',\n",
       " 'anything',\n",
       " 'gregor',\n",
       " 'answered',\n",
       " 'sides',\n",
       " 'im',\n",
       " 'ready',\n",
       " 'making',\n",
       " 'effort',\n",
       " 'remove',\n",
       " 'strangeness',\n",
       " 'voice',\n",
       " 'enunciating',\n",
       " 'carefully',\n",
       " 'putting',\n",
       " 'long',\n",
       " 'pauses',\n",
       " 'individual',\n",
       " 'word',\n",
       " 'father',\n",
       " 'went',\n",
       " 'back',\n",
       " 'breakfast',\n",
       " 'sister',\n",
       " 'whispered',\n",
       " 'gregor',\n",
       " 'open',\n",
       " 'door',\n",
       " 'beg',\n",
       " 'gregor',\n",
       " 'however',\n",
       " 'thought',\n",
       " 'opening',\n",
       " 'door',\n",
       " 'instead',\n",
       " 'congratulated',\n",
       " 'cautious',\n",
       " 'habit',\n",
       " 'acquired',\n",
       " 'travelling',\n",
       " 'locking',\n",
       " 'doors',\n",
       " 'night',\n",
       " 'even',\n",
       " 'home',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'wanted',\n",
       " 'get',\n",
       " 'peace',\n",
       " 'without',\n",
       " 'disturbed',\n",
       " 'get',\n",
       " 'dressed',\n",
       " 'breakfast',\n",
       " 'would',\n",
       " 'consider',\n",
       " 'next',\n",
       " 'well',\n",
       " 'aware',\n",
       " 'would',\n",
       " 'bring',\n",
       " 'thoughts',\n",
       " 'sensible',\n",
       " 'conclusions',\n",
       " 'lying',\n",
       " 'bed',\n",
       " 'remembered',\n",
       " 'often',\n",
       " 'felt',\n",
       " 'slight',\n",
       " 'pain',\n",
       " 'bed',\n",
       " 'perhaps',\n",
       " 'caused',\n",
       " 'lying',\n",
       " 'awkwardly',\n",
       " 'always',\n",
       " 'turned',\n",
       " 'pure',\n",
       " 'imagination',\n",
       " 'wondered',\n",
       " 'imaginings',\n",
       " 'would',\n",
       " 'slowly',\n",
       " 'resolve',\n",
       " 'today',\n",
       " 'slightest',\n",
       " 'doubt',\n",
       " 'change',\n",
       " 'voice',\n",
       " 'nothing',\n",
       " 'first',\n",
       " 'sign',\n",
       " 'serious',\n",
       " 'cold',\n",
       " 'occupational',\n",
       " 'hazard',\n",
       " 'travelling',\n",
       " 'salesmen',\n",
       " 'simple',\n",
       " 'matter',\n",
       " 'throw',\n",
       " 'covers',\n",
       " 'blow',\n",
       " 'little',\n",
       " 'fell',\n",
       " 'became',\n",
       " 'difficult',\n",
       " 'especially',\n",
       " 'exceptionally',\n",
       " 'broad',\n",
       " 'would',\n",
       " 'used',\n",
       " 'arms',\n",
       " 'hands',\n",
       " 'push',\n",
       " 'instead',\n",
       " 'little',\n",
       " 'legs',\n",
       " 'continuously',\n",
       " 'moving',\n",
       " 'different',\n",
       " 'directions',\n",
       " 'moreover',\n",
       " 'unable',\n",
       " 'control',\n",
       " 'wanted',\n",
       " 'bend',\n",
       " 'one',\n",
       " 'first',\n",
       " 'one',\n",
       " 'would',\n",
       " 'stretch',\n",
       " 'finally',\n",
       " 'managed',\n",
       " 'wanted',\n",
       " 'leg',\n",
       " 'others',\n",
       " 'seemed',\n",
       " 'set',\n",
       " 'free',\n",
       " 'would',\n",
       " 'move',\n",
       " 'painfully',\n",
       " 'something',\n",
       " 'cant',\n",
       " 'done',\n",
       " 'bed',\n",
       " 'gregor',\n",
       " 'said',\n",
       " 'dont',\n",
       " 'keep',\n",
       " 'trying',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'wanted',\n",
       " 'get',\n",
       " 'lower',\n",
       " 'part',\n",
       " 'body',\n",
       " 'bed',\n",
       " 'never',\n",
       " 'seen',\n",
       " 'lower',\n",
       " 'part',\n",
       " 'could',\n",
       " 'imagine',\n",
       " 'looked',\n",
       " 'like',\n",
       " 'turned',\n",
       " 'hard',\n",
       " 'move',\n",
       " 'went',\n",
       " 'slowly',\n",
       " 'finally',\n",
       " 'almost',\n",
       " 'frenzy',\n",
       " 'carelessly',\n",
       " 'shoved',\n",
       " 'forwards',\n",
       " 'force',\n",
       " 'could',\n",
       " 'gather',\n",
       " 'chose',\n",
       " 'wrong',\n",
       " 'direction',\n",
       " 'hit',\n",
       " 'hard',\n",
       " 'lower',\n",
       " 'bedpost',\n",
       " 'learned',\n",
       " 'burning',\n",
       " 'pain',\n",
       " 'felt',\n",
       " 'lower',\n",
       " 'part',\n",
       " 'body',\n",
       " 'might',\n",
       " 'well',\n",
       " 'present',\n",
       " 'sensitive',\n",
       " 'tried',\n",
       " 'get',\n",
       " 'top',\n",
       " 'part',\n",
       " 'body',\n",
       " 'bed',\n",
       " 'first',\n",
       " 'carefully',\n",
       " 'turning',\n",
       " 'head',\n",
       " 'side',\n",
       " 'managed',\n",
       " 'quite',\n",
       " 'easily',\n",
       " 'despite',\n",
       " 'breadth',\n",
       " 'weight',\n",
       " 'bulk',\n",
       " 'body',\n",
       " 'eventually',\n",
       " 'followed',\n",
       " 'slowly',\n",
       " 'direction',\n",
       " 'head',\n",
       " 'last',\n",
       " 'got',\n",
       " 'head',\n",
       " 'bed',\n",
       " 'fresh',\n",
       " 'air',\n",
       " 'occurred',\n",
       " 'let',\n",
       " 'fall',\n",
       " 'would',\n",
       " 'miracle',\n",
       " 'head',\n",
       " 'injured',\n",
       " 'became',\n",
       " 'afraid',\n",
       " 'carry',\n",
       " 'pushing',\n",
       " 'forward',\n",
       " 'way',\n",
       " 'could',\n",
       " 'knock',\n",
       " 'price',\n",
       " 'better',\n",
       " 'stay',\n",
       " 'bed',\n",
       " 'lose',\n",
       " 'consciousness',\n",
       " 'took',\n",
       " 'much',\n",
       " 'effort',\n",
       " 'get',\n",
       " 'back',\n",
       " 'earlier',\n",
       " 'lay',\n",
       " 'sighing',\n",
       " 'watching',\n",
       " 'legs',\n",
       " 'struggled',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'possible',\n",
       " 'could',\n",
       " 'think',\n",
       " 'way',\n",
       " 'bringing',\n",
       " 'peace',\n",
       " 'order',\n",
       " 'chaos',\n",
       " 'told',\n",
       " 'possible',\n",
       " 'stay',\n",
       " 'bed',\n",
       " 'sensible',\n",
       " 'thing',\n",
       " 'would',\n",
       " 'get',\n",
       " 'free',\n",
       " 'whatever',\n",
       " 'way',\n",
       " 'could',\n",
       " 'whatever',\n",
       " 'sacrifice',\n",
       " 'time',\n",
       " 'though',\n",
       " 'forget',\n",
       " 'remind',\n",
       " 'calm',\n",
       " 'consideration',\n",
       " 'much',\n",
       " 'better',\n",
       " 'rushing',\n",
       " 'desperate',\n",
       " 'conclusions',\n",
       " 'times',\n",
       " 'like',\n",
       " 'would',\n",
       " 'direct',\n",
       " 'eyes',\n",
       " 'window',\n",
       " 'look',\n",
       " 'clearly',\n",
       " 'could',\n",
       " 'unfortunately',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metamorph_tokens_stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'use', 'anyon', 'anywher', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi', 'give', 'away', 'use', 'term', 'licens', 'includ', 'onlin', 'www', 'net', 'copyright', 'detail', 'pleas', 'follow', 'guidelin', 'file', 'titl', 'metamorphosi', 'author', 'franz', 'kafka', 'translat', 'david', 'wylli', 'releas', 'date', 'august', 'first', 'post', 'may', 'last', 'updat', 'may', 'languag', 'english', 'start', 'metamorphosi', 'c', 'david', 'wylli', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armour', 'like', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slight', 'dome', 'divid', 'arch', 'stiff', 'section', 'bed', 'hard', 'abl', 'cover', 'seem', 'readi', 'slide', 'moment', 'mani', 'leg', 'piti']\n"
     ]
    }
   ],
   "source": [
    "meta_stemmed = [stemmer.stem(word) for word in metamorph_tokens_stopped]\n",
    "print(meta_stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Resources](https://www.guru99.com/stemming-lemmatization-python-nltk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rocky\n",
      "corpora : corpus\n",
      "better : bettor\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocky\", pos='a'))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"bettor\", pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('metamorphosis', 'NN'),\n",
       " ('franz', 'NN'),\n",
       " ('kafka', 'NN'),\n",
       " ('translated', 'VBD'),\n",
       " ('david', 'JJ'),\n",
       " ('wyllie', 'NN'),\n",
       " ('use', 'NN'),\n",
       " ('anyone', 'NN'),\n",
       " ('anywhere', 'RB'),\n",
       " ('cost', 'VBZ')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(metamorph_tokens_stopped)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorph_lemmas_pos = []\n",
    "for x, y in nltk.pos_tag(metamorph_tokens_stopped):\n",
    "    metamorph_lemmas_pos.append((x, get_wordnet_pos(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('metamorphosis', 'n'),\n",
       " ('franz', 'n'),\n",
       " ('kafka', 'n'),\n",
       " ('translated', 'v'),\n",
       " ('david', 'a'),\n",
       " ('wyllie', 'n'),\n",
       " ('use', 'n'),\n",
       " ('anyone', 'n'),\n",
       " ('anywhere', 'r'),\n",
       " ('cost', 'v'),\n",
       " ('almost', 'r'),\n",
       " ('restrictions', 'n'),\n",
       " ('whatsoever', 'v'),\n",
       " ('may', 'n'),\n",
       " ('copy', 'v'),\n",
       " ('give', 'v'),\n",
       " ('away', 'r'),\n",
       " ('use', 'n'),\n",
       " ('terms', 'n'),\n",
       " ('license', 'n'),\n",
       " ('included', 'v'),\n",
       " ('online', 'a'),\n",
       " ('www', 'a'),\n",
       " ('net', 'n'),\n",
       " ('copyrighted', 'v'),\n",
       " ('details', 'n'),\n",
       " ('please', 'v'),\n",
       " ('follow', 'a'),\n",
       " ('guidelines', 'n'),\n",
       " ('file', 'n'),\n",
       " ('title', 'n'),\n",
       " ('metamorphosis', 'n'),\n",
       " ('author', 'n'),\n",
       " ('franz', 'n'),\n",
       " ('kafka', 'n'),\n",
       " ('translator', 'n'),\n",
       " ('david', 'n'),\n",
       " ('wyllie', 'n'),\n",
       " ('release', 'n'),\n",
       " ('date', 'n'),\n",
       " ('august', 'n'),\n",
       " ('first', 'r'),\n",
       " ('posted', 'v'),\n",
       " ('may', 'n'),\n",
       " ('last', 'a'),\n",
       " ('updated', 'a'),\n",
       " ('may', 'n'),\n",
       " ('language', 'n'),\n",
       " ('english', 'v'),\n",
       " ('start', 'a'),\n",
       " ('metamorphosis', 'n'),\n",
       " ('c', 'n'),\n",
       " ('david', 'n'),\n",
       " ('wyllie', 'n'),\n",
       " ('metamorphosis', 'n'),\n",
       " ('franz', 'n'),\n",
       " ('kafka', 'n'),\n",
       " ('translated', 'v'),\n",
       " ('david', 'a'),\n",
       " ('wyllie', 'n'),\n",
       " ('one', 'n'),\n",
       " ('morning', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('samsa', 'n'),\n",
       " ('woke', 'v'),\n",
       " ('troubled', 'a'),\n",
       " ('dreams', 'n'),\n",
       " ('found', 'v'),\n",
       " ('transformed', 'v'),\n",
       " ('bed', 'n'),\n",
       " ('horrible', 'a'),\n",
       " ('vermin', 'n'),\n",
       " ('lay', 'v'),\n",
       " ('armour', 'n'),\n",
       " ('like', 'n'),\n",
       " ('back', 'r'),\n",
       " ('lifted', 'v'),\n",
       " ('head', 'n'),\n",
       " ('little', 'n'),\n",
       " ('could', 'n'),\n",
       " ('see', 'v'),\n",
       " ('brown', 'a'),\n",
       " ('belly', 'r'),\n",
       " ('slightly', 'r'),\n",
       " ('domed', 'v'),\n",
       " ('divided', 'a'),\n",
       " ('arches', 'n'),\n",
       " ('stiff', 'a'),\n",
       " ('sections', 'n'),\n",
       " ('bedding', 'v'),\n",
       " ('hardly', 'r'),\n",
       " ('able', 'a'),\n",
       " ('cover', 'n'),\n",
       " ('seemed', 'v'),\n",
       " ('ready', 'a'),\n",
       " ('slide', 'a'),\n",
       " ('moment', 'n'),\n",
       " ('many', 'a'),\n",
       " ('legs', 'n'),\n",
       " ('pitifully', 'r'),\n",
       " ('thin', 'a'),\n",
       " ('compared', 'v'),\n",
       " ('size', 'n'),\n",
       " ('rest', 'n'),\n",
       " ('waved', 'v'),\n",
       " ('helplessly', 'r'),\n",
       " ('looked', 'v'),\n",
       " ('whats', 'n'),\n",
       " ('happened', 'v'),\n",
       " ('thought', 'v'),\n",
       " ('wasnt', 'a'),\n",
       " ('dream', 'n'),\n",
       " ('room', 'n'),\n",
       " ('proper', 'n'),\n",
       " ('human', 'a'),\n",
       " ('room', 'n'),\n",
       " ('although', 'n'),\n",
       " ('little', 'a'),\n",
       " ('small', 'a'),\n",
       " ('lay', 'v'),\n",
       " ('peacefully', 'r'),\n",
       " ('four', 'n'),\n",
       " ('familiar', 'a'),\n",
       " ('walls', 'n'),\n",
       " ('collection', 'n'),\n",
       " ('textile', 'n'),\n",
       " ('samples', 'n'),\n",
       " ('lay', 'v'),\n",
       " ('spread', 'n'),\n",
       " ('table', 'n'),\n",
       " ('samsa', 'n'),\n",
       " ('travelling', 'v'),\n",
       " ('salesman', 'a'),\n",
       " ('hung', 'a'),\n",
       " ('picture', 'n'),\n",
       " ('recently', 'r'),\n",
       " ('cut', 'v'),\n",
       " ('illustrated', 'a'),\n",
       " ('magazine', 'n'),\n",
       " ('housed', 'v'),\n",
       " ('nice', 'r'),\n",
       " ('gilded', 'v'),\n",
       " ('frame', 'n'),\n",
       " ('showed', 'v'),\n",
       " ('lady', 'a'),\n",
       " ('fitted', 'v'),\n",
       " ('fur', 'n'),\n",
       " ('hat', 'n'),\n",
       " ('fur', 'v'),\n",
       " ('boa', 'n'),\n",
       " ('sat', 'v'),\n",
       " ('upright', 'a'),\n",
       " ('raising', 'v'),\n",
       " ('heavy', 'a'),\n",
       " ('fur', 'a'),\n",
       " ('muff', 'n'),\n",
       " ('covered', 'v'),\n",
       " ('whole', 'a'),\n",
       " ('lower', 'a'),\n",
       " ('arm', 'n'),\n",
       " ('towards', 'n'),\n",
       " ('viewer', 'v'),\n",
       " ('gregor', 'n'),\n",
       " ('turned', 'v'),\n",
       " ('look', 'n'),\n",
       " ('window', 'n'),\n",
       " ('dull', 'n'),\n",
       " ('weather', 'n'),\n",
       " ('drops', 'n'),\n",
       " ('rain', 'n'),\n",
       " ('could', 'n'),\n",
       " ('heard', 'v'),\n",
       " ('hitting', 'v'),\n",
       " ('pane', 'n'),\n",
       " ('made', 'v'),\n",
       " ('feel', 'n'),\n",
       " ('quite', 'r'),\n",
       " ('sad', 'a'),\n",
       " ('sleep', 'a'),\n",
       " ('little', 'a'),\n",
       " ('bit', 'n'),\n",
       " ('longer', 'r'),\n",
       " ('forget', 'v'),\n",
       " ('nonsense', 'a'),\n",
       " ('thought', 'n'),\n",
       " ('something', 'n'),\n",
       " ('unable', 'a'),\n",
       " ('used', 'v'),\n",
       " ('sleeping', 'v'),\n",
       " ('right', 'a'),\n",
       " ('present', 'a'),\n",
       " ('state', 'n'),\n",
       " ('couldnt', 'n'),\n",
       " ('get', 'n'),\n",
       " ('position', 'n'),\n",
       " ('however', 'r'),\n",
       " ('hard', 'a'),\n",
       " ('threw', 'v'),\n",
       " ('onto', 'r'),\n",
       " ('right', 'a'),\n",
       " ('always', 'r'),\n",
       " ('rolled', 'v'),\n",
       " ('back', 'r'),\n",
       " ('must', 'n'),\n",
       " ('tried', 'v'),\n",
       " ('hundred', 'n'),\n",
       " ('times', 'n'),\n",
       " ('shut', 'a'),\n",
       " ('eyes', 'n'),\n",
       " ('wouldnt', 'v'),\n",
       " ('look', 'v'),\n",
       " ('floundering', 'v'),\n",
       " ('legs', 'n'),\n",
       " ('stopped', 'v'),\n",
       " ('began', 'v'),\n",
       " ('feel', 'a'),\n",
       " ('mild', 'a'),\n",
       " ('dull', 'n'),\n",
       " ('pain', 'n'),\n",
       " ('never', 'r'),\n",
       " ('felt', 'v'),\n",
       " ('oh', 'a'),\n",
       " ('god', 'n'),\n",
       " ('thought', 'v'),\n",
       " ('strenuous', 'a'),\n",
       " ('career', 'n'),\n",
       " ('ive', 'a'),\n",
       " ('chosen', 'n'),\n",
       " ('travelling', 'v'),\n",
       " ('day', 'n'),\n",
       " ('day', 'n'),\n",
       " ('business', 'n'),\n",
       " ('like', 'n'),\n",
       " ('takes', 'v'),\n",
       " ('much', 'a'),\n",
       " ('effort', 'n'),\n",
       " ('business', 'n'),\n",
       " ('home', 'n'),\n",
       " ('top', 'n'),\n",
       " ('theres', 'n'),\n",
       " ('curse', 'v'),\n",
       " ('travelling', 'v'),\n",
       " ('worries', 'n'),\n",
       " ('making', 'v'),\n",
       " ('train', 'a'),\n",
       " ('connections', 'n'),\n",
       " ('bad', 'a'),\n",
       " ('irregular', 'a'),\n",
       " ('food', 'n'),\n",
       " ('contact', 'n'),\n",
       " ('different', 'a'),\n",
       " ('people', 'n'),\n",
       " ('time', 'n'),\n",
       " ('never', 'r'),\n",
       " ('get', 'v'),\n",
       " ('know', 'a'),\n",
       " ('anyone', 'n'),\n",
       " ('become', 'n'),\n",
       " ('friendly', 'a'),\n",
       " ('go', 'v'),\n",
       " ('hell', 'n'),\n",
       " ('felt', 'v'),\n",
       " ('slight', 'a'),\n",
       " ('itch', 'n'),\n",
       " ('belly', 'r'),\n",
       " ('pushed', 'v'),\n",
       " ('slowly', 'r'),\n",
       " ('back', 'r'),\n",
       " ('towards', 'n'),\n",
       " ('headboard', 'n'),\n",
       " ('could', 'n'),\n",
       " ('lift', 'v'),\n",
       " ('head', 'n'),\n",
       " ('better', 'r'),\n",
       " ('found', 'v'),\n",
       " ('itch', 'n'),\n",
       " ('saw', 'n'),\n",
       " ('covered', 'v'),\n",
       " ('lots', 'a'),\n",
       " ('little', 'a'),\n",
       " ('white', 'a'),\n",
       " ('spots', 'n'),\n",
       " ('didnt', 'v'),\n",
       " ('know', 'v'),\n",
       " ('make', 'v'),\n",
       " ('tried', 'a'),\n",
       " ('feel', 'n'),\n",
       " ('place', 'n'),\n",
       " ('one', 'n'),\n",
       " ('legs', 'n'),\n",
       " ('drew', 'v'),\n",
       " ('quickly', 'r'),\n",
       " ('back', 'r'),\n",
       " ('soon', 'r'),\n",
       " ('touched', 'v'),\n",
       " ('overcome', 'a'),\n",
       " ('cold', 'a'),\n",
       " ('shudder', 'n'),\n",
       " ('slid', 'v'),\n",
       " ('back', 'r'),\n",
       " ('former', 'a'),\n",
       " ('position', 'n'),\n",
       " ('getting', 'v'),\n",
       " ('early', 'a'),\n",
       " ('time', 'n'),\n",
       " ('thought', 'v'),\n",
       " ('makes', 'v'),\n",
       " ('stupid', 'a'),\n",
       " ('youve', 'n'),\n",
       " ('got', 'v'),\n",
       " ('get', 'v'),\n",
       " ('enough', 'a'),\n",
       " ('sleep', 'n'),\n",
       " ('travelling', 'v'),\n",
       " ('salesmen', 'n'),\n",
       " ('live', 'a'),\n",
       " ('life', 'n'),\n",
       " ('luxury', 'n'),\n",
       " ('instance', 'n'),\n",
       " ('whenever', 'n'),\n",
       " ('go', 'v'),\n",
       " ('back', 'r'),\n",
       " ('guest', 'a'),\n",
       " ('house', 'n'),\n",
       " ('morning', 'n'),\n",
       " ('copy', 'n'),\n",
       " ('contract', 'n'),\n",
       " ('gentlemen', 'n'),\n",
       " ('always', 'r'),\n",
       " ('still', 'r'),\n",
       " ('sitting', 'v'),\n",
       " ('eating', 'v'),\n",
       " ('breakfasts', 'n'),\n",
       " ('ought', 'n'),\n",
       " ('try', 'v'),\n",
       " ('boss', 'n'),\n",
       " ('id', 'n'),\n",
       " ('get', 'n'),\n",
       " ('kicked', 'v'),\n",
       " ('spot', 'n'),\n",
       " ('knows', 'n'),\n",
       " ('maybe', 'r'),\n",
       " ('would', 'n'),\n",
       " ('best', 'v'),\n",
       " ('thing', 'n'),\n",
       " ('didnt', 'n'),\n",
       " ('parents', 'n'),\n",
       " ('think', 'v'),\n",
       " ('id', 'v'),\n",
       " ('given', 'v'),\n",
       " ('notice', 'r'),\n",
       " ('long', 'a'),\n",
       " ('time', 'n'),\n",
       " ('ago', 'n'),\n",
       " ('id', 'n'),\n",
       " ('gone', 'v'),\n",
       " ('boss', 'r'),\n",
       " ('told', 'a'),\n",
       " ('think', 'v'),\n",
       " ('tell', 'n'),\n",
       " ('everything', 'n'),\n",
       " ('would', 'n'),\n",
       " ('let', 'v'),\n",
       " ('know', 'v'),\n",
       " ('feel', 'v'),\n",
       " ('hed', 'a'),\n",
       " ('fall', 'n'),\n",
       " ('right', 'r'),\n",
       " ('desk', 'n'),\n",
       " ('funny', 'a'),\n",
       " ('sort', 'n'),\n",
       " ('business', 'n'),\n",
       " ('sitting', 'v'),\n",
       " ('desk', 'n'),\n",
       " ('talking', 'v'),\n",
       " ('subordinates', 'n'),\n",
       " ('especially', 'r'),\n",
       " ('go', 'v'),\n",
       " ('right', 'a'),\n",
       " ('close', 'r'),\n",
       " ('boss', 'n'),\n",
       " ('hard', 'a'),\n",
       " ('hearing', 'n'),\n",
       " ('well', 'n'),\n",
       " ('theres', 'n'),\n",
       " ('still', 'r'),\n",
       " ('hope', 'v'),\n",
       " ('ive', 'a'),\n",
       " ('got', 'v'),\n",
       " ('money', 'n'),\n",
       " ('together', 'r'),\n",
       " ('pay', 'n'),\n",
       " ('parents', 'n'),\n",
       " ('debt', 'n'),\n",
       " ('another', 'n'),\n",
       " ('five', 'n'),\n",
       " ('six', 'n'),\n",
       " ('years', 'n'),\n",
       " ('suppose', 'a'),\n",
       " ('thats', 'n'),\n",
       " ('definitely', 'r'),\n",
       " ('ill', 'a'),\n",
       " ('thats', 'n'),\n",
       " ('ill', 'v'),\n",
       " ('make', 'v'),\n",
       " ('big', 'a'),\n",
       " ('change', 'n'),\n",
       " ('first', 'r'),\n",
       " ('though', 'n'),\n",
       " ('ive', 'a'),\n",
       " ('got', 'v'),\n",
       " ('get', 'v'),\n",
       " ('train', 'a'),\n",
       " ('leaves', 'n'),\n",
       " ('five', 'n'),\n",
       " ('looked', 'v'),\n",
       " ('alarm', 'n'),\n",
       " ('clock', 'n'),\n",
       " ('ticking', 'v'),\n",
       " ('chest', 'a'),\n",
       " ('drawers', 'n'),\n",
       " ('god', 'v'),\n",
       " ('heaven', 'r'),\n",
       " ('thought', 'v'),\n",
       " ('half', 'n'),\n",
       " ('past', 'n'),\n",
       " ('six', 'n'),\n",
       " ('hands', 'n'),\n",
       " ('quietly', 'r'),\n",
       " ('moving', 'v'),\n",
       " ('forwards', 'n'),\n",
       " ('even', 'r'),\n",
       " ('later', 'r'),\n",
       " ('half', 'a'),\n",
       " ('past', 'n'),\n",
       " ('like', 'a'),\n",
       " ('quarter', 'n'),\n",
       " ('seven', 'n'),\n",
       " ('alarm', 'n'),\n",
       " ('clock', 'n'),\n",
       " ('rung', 'n'),\n",
       " ('could', 'n'),\n",
       " ('see', 'v'),\n",
       " ('bed', 'a'),\n",
       " ('set', 'v'),\n",
       " ('four', 'n'),\n",
       " ('oclock', 'n'),\n",
       " ('certainly', 'r'),\n",
       " ('must', 'n'),\n",
       " ('rung', 'v'),\n",
       " ('yes', 'n'),\n",
       " ('possible', 'a'),\n",
       " ('quietly', 'r'),\n",
       " ('sleep', 'a'),\n",
       " ('furniture', 'n'),\n",
       " ('rattling', 'v'),\n",
       " ('noise', 'r'),\n",
       " ('true', 'a'),\n",
       " ('slept', 'n'),\n",
       " ('peacefully', 'r'),\n",
       " ('probably', 'r'),\n",
       " ('deeply', 'r'),\n",
       " ('next', 'a'),\n",
       " ('train', 'n'),\n",
       " ('went', 'v'),\n",
       " ('seven', 'n'),\n",
       " ('catch', 'n'),\n",
       " ('would', 'n'),\n",
       " ('rush', 'v'),\n",
       " ('like', 'n'),\n",
       " ('mad', 'a'),\n",
       " ('collection', 'n'),\n",
       " ('samples', 'n'),\n",
       " ('still', 'r'),\n",
       " ('packed', 'v'),\n",
       " ('feel', 'a'),\n",
       " ('particularly', 'r'),\n",
       " ('fresh', 'a'),\n",
       " ('lively', 'r'),\n",
       " ('even', 'r'),\n",
       " ('catch', 'v'),\n",
       " ('train', 'n'),\n",
       " ('would', 'n'),\n",
       " ('avoid', 'v'),\n",
       " ('bosss', 'n'),\n",
       " ('anger', 'n'),\n",
       " ('office', 'n'),\n",
       " ('assistant', 'n'),\n",
       " ('would', 'n'),\n",
       " ('see', 'v'),\n",
       " ('five', 'n'),\n",
       " ('oclock', 'n'),\n",
       " ('train', 'n'),\n",
       " ('go', 'v'),\n",
       " ('would', 'n'),\n",
       " ('put', 'v'),\n",
       " ('report', 'n'),\n",
       " ('gregors', 'n'),\n",
       " ('long', 'a'),\n",
       " ('time', 'n'),\n",
       " ('ago', 'n'),\n",
       " ('office', 'n'),\n",
       " ('assistant', 'n'),\n",
       " ('bosss', 'n'),\n",
       " ('man', 'n'),\n",
       " ('spineless', 'n'),\n",
       " ('understanding', 'v'),\n",
       " ('reported', 'v'),\n",
       " ('sick', 'a'),\n",
       " ('would', 'n'),\n",
       " ('extremely', 'r'),\n",
       " ('strained', 'v'),\n",
       " ('suspicious', 'a'),\n",
       " ('fifteen', 'a'),\n",
       " ('years', 'n'),\n",
       " ('service', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('never', 'r'),\n",
       " ('yet', 'r'),\n",
       " ('ill', 'a'),\n",
       " ('boss', 'n'),\n",
       " ('would', 'n'),\n",
       " ('certainly', 'r'),\n",
       " ('come', 'v'),\n",
       " ('round', 'n'),\n",
       " ('doctor', 'n'),\n",
       " ('medical', 'a'),\n",
       " ('insurance', 'n'),\n",
       " ('company', 'n'),\n",
       " ('accuse', 'n'),\n",
       " ('parents', 'n'),\n",
       " ('lazy', 'a'),\n",
       " ('son', 'n'),\n",
       " ('accept', 'v'),\n",
       " ('doctors', 'n'),\n",
       " ('recommendation', 'v'),\n",
       " ('make', 'v'),\n",
       " ('claim', 'n'),\n",
       " ('doctor', 'n'),\n",
       " ('believed', 'v'),\n",
       " ('one', 'n'),\n",
       " ('ever', 'r'),\n",
       " ('ill', 'v'),\n",
       " ('many', 'a'),\n",
       " ('workshy', 'a'),\n",
       " ('whats', 'n'),\n",
       " ('would', 'n'),\n",
       " ('entirely', 'r'),\n",
       " ('wrong', 'a'),\n",
       " ('case', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('fact', 'n'),\n",
       " ('apart', 'r'),\n",
       " ('excessive', 'a'),\n",
       " ('sleepiness', 'n'),\n",
       " ('sleeping', 'v'),\n",
       " ('long', 'a'),\n",
       " ('feel', 'n'),\n",
       " ('completely', 'r'),\n",
       " ('well', 'r'),\n",
       " ('even', 'r'),\n",
       " ('felt', 'v'),\n",
       " ('much', 'a'),\n",
       " ('hungrier', 'a'),\n",
       " ('usual', 'a'),\n",
       " ('still', 'r'),\n",
       " ('hurriedly', 'r'),\n",
       " ('thinking', 'v'),\n",
       " ('unable', 'a'),\n",
       " ('decide', 'a'),\n",
       " ('get', 'n'),\n",
       " ('bed', 'v'),\n",
       " ('clock', 'a'),\n",
       " ('struck', 'a'),\n",
       " ('quarter', 'n'),\n",
       " ('seven', 'n'),\n",
       " ('cautious', 'a'),\n",
       " ('knock', 'n'),\n",
       " ('door', 'n'),\n",
       " ('near', 'n'),\n",
       " ('head', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('somebody', 'n'),\n",
       " ('called', 'v'),\n",
       " ('mother', 'r'),\n",
       " ('quarter', 'n'),\n",
       " ('seven', 'n'),\n",
       " ('didnt', 'n'),\n",
       " ('want', 'v'),\n",
       " ('go', 'v'),\n",
       " ('somewhere', 'r'),\n",
       " ('gentle', 'a'),\n",
       " ('voice', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('shocked', 'v'),\n",
       " ('heard', 'a'),\n",
       " ('voice', 'n'),\n",
       " ('answering', 'v'),\n",
       " ('could', 'n'),\n",
       " ('hardly', 'r'),\n",
       " ('recognised', 'a'),\n",
       " ('voice', 'n'),\n",
       " ('deep', 'a'),\n",
       " ('inside', 'r'),\n",
       " ('painful', 'a'),\n",
       " ('uncontrollable', 'a'),\n",
       " ('squeaking', 'v'),\n",
       " ('mixed', 'a'),\n",
       " ('words', 'n'),\n",
       " ('could', 'n'),\n",
       " ('made', 'v'),\n",
       " ('first', 'a'),\n",
       " ('sort', 'n'),\n",
       " ('echo', 'n'),\n",
       " ('made', 'v'),\n",
       " ('unclear', 'a'),\n",
       " ('leaving', 'a'),\n",
       " ('hearer', 'n'),\n",
       " ('unsure', 'n'),\n",
       " ('whether', 'n'),\n",
       " ('heard', 'n'),\n",
       " ('properly', 'r'),\n",
       " ('gregor', 'r'),\n",
       " ('wanted', 'v'),\n",
       " ('give', 'a'),\n",
       " ('full', 'a'),\n",
       " ('answer', 'n'),\n",
       " ('explain', 'n'),\n",
       " ('everything', 'n'),\n",
       " ('circumstances', 'n'),\n",
       " ('contented', 'v'),\n",
       " ('saying', 'v'),\n",
       " ('yes', 'r'),\n",
       " ('mother', 'r'),\n",
       " ('yes', 'a'),\n",
       " ('thank', 'n'),\n",
       " ('im', 'n'),\n",
       " ('getting', 'v'),\n",
       " ('change', 'n'),\n",
       " ('gregors', 'n'),\n",
       " ('voice', 'n'),\n",
       " ('probably', 'r'),\n",
       " ('could', 'n'),\n",
       " ('noticed', 'v'),\n",
       " ('outside', 'a'),\n",
       " ('wooden', 'a'),\n",
       " ('door', 'n'),\n",
       " ('mother', 'n'),\n",
       " ('satisfied', 'v'),\n",
       " ('explanation', 'n'),\n",
       " ('shuffled', 'v'),\n",
       " ('away', 'r'),\n",
       " ('short', 'a'),\n",
       " ('conversation', 'n'),\n",
       " ('made', 'v'),\n",
       " ('members', 'n'),\n",
       " ('family', 'n'),\n",
       " ('aware', 'a'),\n",
       " ('gregor', 'n'),\n",
       " ('expectations', 'n'),\n",
       " ('still', 'r'),\n",
       " ('home', 'v'),\n",
       " ('soon', 'r'),\n",
       " ('father', 'r'),\n",
       " ('came', 'v'),\n",
       " ('knocking', 'v'),\n",
       " ('one', 'n'),\n",
       " ('side', 'n'),\n",
       " ('doors', 'n'),\n",
       " ('gently', 'r'),\n",
       " ('fist', 'v'),\n",
       " ('gregor', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('called', 'v'),\n",
       " ('whats', 'n'),\n",
       " ('wrong', 'a'),\n",
       " ('short', 'a'),\n",
       " ('called', 'v'),\n",
       " ('warning', 'v'),\n",
       " ('deepness', 'n'),\n",
       " ('voice', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('gregor', 'a'),\n",
       " ('side', 'n'),\n",
       " ('door', 'n'),\n",
       " ('sister', 'n'),\n",
       " ('came', 'v'),\n",
       " ('plaintively', 'r'),\n",
       " ('gregor', 'a'),\n",
       " ('arent', 'n'),\n",
       " ('well', 'r'),\n",
       " ('need', 'v'),\n",
       " ('anything', 'n'),\n",
       " ('gregor', 'a'),\n",
       " ('answered', 'v'),\n",
       " ('sides', 'n'),\n",
       " ('im', 'v'),\n",
       " ('ready', 'a'),\n",
       " ('making', 'v'),\n",
       " ('effort', 'n'),\n",
       " ('remove', 'v'),\n",
       " ('strangeness', 'a'),\n",
       " ('voice', 'n'),\n",
       " ('enunciating', 'v'),\n",
       " ('carefully', 'r'),\n",
       " ('putting', 'v'),\n",
       " ('long', 'a'),\n",
       " ('pauses', 'n'),\n",
       " ('individual', 'a'),\n",
       " ('word', 'n'),\n",
       " ('father', 'n'),\n",
       " ('went', 'v'),\n",
       " ('back', 'r'),\n",
       " ('breakfast', 'n'),\n",
       " ('sister', 'n'),\n",
       " ('whispered', 'v'),\n",
       " ('gregor', 'a'),\n",
       " ('open', 'a'),\n",
       " ('door', 'n'),\n",
       " ('beg', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('however', 'r'),\n",
       " ('thought', 'v'),\n",
       " ('opening', 'v'),\n",
       " ('door', 'n'),\n",
       " ('instead', 'r'),\n",
       " ('congratulated', 'v'),\n",
       " ('cautious', 'a'),\n",
       " ('habit', 'n'),\n",
       " ('acquired', 'v'),\n",
       " ('travelling', 'v'),\n",
       " ('locking', 'v'),\n",
       " ('doors', 'n'),\n",
       " ('night', 'n'),\n",
       " ('even', 'r'),\n",
       " ('home', 'v'),\n",
       " ('first', 'a'),\n",
       " ('thing', 'n'),\n",
       " ('wanted', 'v'),\n",
       " ('get', 'v'),\n",
       " ('peace', 'n'),\n",
       " ('without', 'n'),\n",
       " ('disturbed', 'a'),\n",
       " ('get', 'n'),\n",
       " ('dressed', 'v'),\n",
       " ('breakfast', 'n'),\n",
       " ('would', 'n'),\n",
       " ('consider', 'v'),\n",
       " ('next', 'r'),\n",
       " ('well', 'r'),\n",
       " ('aware', 'a'),\n",
       " ('would', 'n'),\n",
       " ('bring', 'v'),\n",
       " ('thoughts', 'n'),\n",
       " ('sensible', 'a'),\n",
       " ('conclusions', 'n'),\n",
       " ('lying', 'v'),\n",
       " ('bed', 'n'),\n",
       " ('remembered', 'v'),\n",
       " ('often', 'r'),\n",
       " ('felt', 'a'),\n",
       " ('slight', 'a'),\n",
       " ('pain', 'n'),\n",
       " ('bed', 'v'),\n",
       " ('perhaps', 'r'),\n",
       " ('caused', 'v'),\n",
       " ('lying', 'v'),\n",
       " ('awkwardly', 'r'),\n",
       " ('always', 'r'),\n",
       " ('turned', 'v'),\n",
       " ('pure', 'a'),\n",
       " ('imagination', 'n'),\n",
       " ('wondered', 'v'),\n",
       " ('imaginings', 'n'),\n",
       " ('would', 'n'),\n",
       " ('slowly', 'r'),\n",
       " ('resolve', 'v'),\n",
       " ('today', 'n'),\n",
       " ('slightest', 'a'),\n",
       " ('doubt', 'n'),\n",
       " ('change', 'n'),\n",
       " ('voice', 'n'),\n",
       " ('nothing', 'n'),\n",
       " ('first', 'r'),\n",
       " ('sign', 'v'),\n",
       " ('serious', 'a'),\n",
       " ('cold', 'a'),\n",
       " ('occupational', 'a'),\n",
       " ('hazard', 'n'),\n",
       " ('travelling', 'v'),\n",
       " ('salesmen', 'n'),\n",
       " ('simple', 'a'),\n",
       " ('matter', 'n'),\n",
       " ('throw', 'n'),\n",
       " ('covers', 'v'),\n",
       " ('blow', 'a'),\n",
       " ('little', 'r'),\n",
       " ('fell', 'v'),\n",
       " ('became', 'v'),\n",
       " ('difficult', 'a'),\n",
       " ('especially', 'r'),\n",
       " ('exceptionally', 'r'),\n",
       " ('broad', 'a'),\n",
       " ('would', 'n'),\n",
       " ('used', 'v'),\n",
       " ('arms', 'n'),\n",
       " ('hands', 'v'),\n",
       " ('push', 'a'),\n",
       " ('instead', 'r'),\n",
       " ('little', 'a'),\n",
       " ('legs', 'a'),\n",
       " ('continuously', 'r'),\n",
       " ('moving', 'v'),\n",
       " ('different', 'a'),\n",
       " ('directions', 'n'),\n",
       " ('moreover', 'r'),\n",
       " ('unable', 'a'),\n",
       " ('control', 'n'),\n",
       " ('wanted', 'v'),\n",
       " ('bend', 'n'),\n",
       " ('one', 'n'),\n",
       " ('first', 'a'),\n",
       " ('one', 'n'),\n",
       " ('would', 'n'),\n",
       " ('stretch', 'v'),\n",
       " ('finally', 'r'),\n",
       " ('managed', 'v'),\n",
       " ('wanted', 'v'),\n",
       " ('leg', 'n'),\n",
       " ('others', 'n'),\n",
       " ('seemed', 'v'),\n",
       " ('set', 'v'),\n",
       " ('free', 'a'),\n",
       " ('would', 'n'),\n",
       " ('move', 'v'),\n",
       " ('painfully', 'r'),\n",
       " ('something', 'n'),\n",
       " ('cant', 'a'),\n",
       " ('done', 'v'),\n",
       " ('bed', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('said', 'v'),\n",
       " ('dont', 'a'),\n",
       " ('keep', 'v'),\n",
       " ('trying', 'v'),\n",
       " ('first', 'a'),\n",
       " ('thing', 'n'),\n",
       " ('wanted', 'v'),\n",
       " ('get', 'v'),\n",
       " ('lower', 'a'),\n",
       " ('part', 'n'),\n",
       " ('body', 'n'),\n",
       " ('bed', 'v'),\n",
       " ('never', 'r'),\n",
       " ('seen', 'v'),\n",
       " ('lower', 'a'),\n",
       " ('part', 'n'),\n",
       " ('could', 'n'),\n",
       " ('imagine', 'v'),\n",
       " ('looked', 'v'),\n",
       " ('like', 'n'),\n",
       " ('turned', 'v'),\n",
       " ('hard', 'r'),\n",
       " ('move', 'n'),\n",
       " ('went', 'v'),\n",
       " ('slowly', 'r'),\n",
       " ('finally', 'r'),\n",
       " ('almost', 'r'),\n",
       " ('frenzy', 'n'),\n",
       " ('carelessly', 'r'),\n",
       " ('shoved', 'v'),\n",
       " ('forwards', 'n'),\n",
       " ('force', 'n'),\n",
       " ('could', 'n'),\n",
       " ('gather', 'v'),\n",
       " ('chose', 'v'),\n",
       " ('wrong', 'a'),\n",
       " ('direction', 'n'),\n",
       " ('hit', 'v'),\n",
       " ('hard', 'a'),\n",
       " ('lower', 'a'),\n",
       " ('bedpost', 'n'),\n",
       " ('learned', 'v'),\n",
       " ('burning', 'v'),\n",
       " ('pain', 'n'),\n",
       " ('felt', 'v'),\n",
       " ('lower', 'a'),\n",
       " ('part', 'n'),\n",
       " ('body', 'n'),\n",
       " ('might', 'n'),\n",
       " ('well', 'r'),\n",
       " ('present', 'v'),\n",
       " ('sensitive', 'a'),\n",
       " ('tried', 'a'),\n",
       " ('get', 'n'),\n",
       " ('top', 'a'),\n",
       " ('part', 'n'),\n",
       " ('body', 'n'),\n",
       " ('bed', 'v'),\n",
       " ('first', 'a'),\n",
       " ('carefully', 'r'),\n",
       " ('turning', 'v'),\n",
       " ('head', 'a'),\n",
       " ('side', 'n'),\n",
       " ('managed', 'v'),\n",
       " ('quite', 'r'),\n",
       " ('easily', 'r'),\n",
       " ('despite', 'n'),\n",
       " ('breadth', 'a'),\n",
       " ('weight', 'n'),\n",
       " ('bulk', 'n'),\n",
       " ('body', 'n'),\n",
       " ('eventually', 'r'),\n",
       " ('followed', 'v'),\n",
       " ('slowly', 'a'),\n",
       " ('direction', 'n'),\n",
       " ('head', 'n'),\n",
       " ('last', 'a'),\n",
       " ('got', 'v'),\n",
       " ('head', 'a'),\n",
       " ('bed', 'n'),\n",
       " ('fresh', 'a'),\n",
       " ('air', 'n'),\n",
       " ('occurred', 'v'),\n",
       " ('let', 'a'),\n",
       " ('fall', 'n'),\n",
       " ('would', 'n'),\n",
       " ('miracle', 'v'),\n",
       " ('head', 'n'),\n",
       " ('injured', 'a'),\n",
       " ('became', 'v'),\n",
       " ('afraid', 'a'),\n",
       " ('carry', 'n'),\n",
       " ('pushing', 'v'),\n",
       " ('forward', 'a'),\n",
       " ('way', 'n'),\n",
       " ('could', 'n'),\n",
       " ('knock', 'v'),\n",
       " ('price', 'n'),\n",
       " ('better', 'r'),\n",
       " ('stay', 'n'),\n",
       " ('bed', 'v'),\n",
       " ('lose', 'a'),\n",
       " ('consciousness', 'n'),\n",
       " ('took', 'v'),\n",
       " ('much', 'a'),\n",
       " ('effort', 'n'),\n",
       " ('get', 'v'),\n",
       " ('back', 'r'),\n",
       " ('earlier', 'r'),\n",
       " ('lay', 'n'),\n",
       " ('sighing', 'v'),\n",
       " ('watching', 'v'),\n",
       " ('legs', 'n'),\n",
       " ('struggled', 'v'),\n",
       " ('even', 'r'),\n",
       " ('harder', 'r'),\n",
       " ('possible', 'a'),\n",
       " ('could', 'n'),\n",
       " ('think', 'v'),\n",
       " ('way', 'n'),\n",
       " ('bringing', 'v'),\n",
       " ('peace', 'n'),\n",
       " ('order', 'n'),\n",
       " ('chaos', 'n'),\n",
       " ('told', 'v'),\n",
       " ('possible', 'a'),\n",
       " ('stay', 'n'),\n",
       " ('bed', 'v'),\n",
       " ('sensible', 'a'),\n",
       " ('thing', 'n'),\n",
       " ('would', 'n'),\n",
       " ('get', 'v'),\n",
       " ('free', 'a'),\n",
       " ('whatever', 'n'),\n",
       " ('way', 'n'),\n",
       " ('could', 'n'),\n",
       " ('whatever', 'v'),\n",
       " ('sacrifice', 'a'),\n",
       " ('time', 'n'),\n",
       " ('though', 'n'),\n",
       " ('forget', 'n'),\n",
       " ('remind', 'v'),\n",
       " ('calm', 'a'),\n",
       " ('consideration', 'n'),\n",
       " ('much', 'r'),\n",
       " ('better', 'r'),\n",
       " ('rushing', 'v'),\n",
       " ('desperate', 'a'),\n",
       " ('conclusions', 'n'),\n",
       " ('times', 'n'),\n",
       " ('like', 'n'),\n",
       " ('would', 'n'),\n",
       " ('direct', 'v'),\n",
       " ('eyes', 'n'),\n",
       " ('window', 'a'),\n",
       " ('look', 'v'),\n",
       " ('clearly', 'r'),\n",
       " ('could', 'n'),\n",
       " ('unfortunately', 'r'),\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metamorph_lemmas_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('metamorphosis', 'n'),\n",
       " ('franz', 'n'),\n",
       " ('kafka', 'n'),\n",
       " ('translated', 'v'),\n",
       " ('david', 'a'),\n",
       " ('wyllie', 'n'),\n",
       " ('use', 'n'),\n",
       " ('anyone', 'n'),\n",
       " ('anywhere', 'r'),\n",
       " ('cost', 'v'),\n",
       " ('almost', 'r'),\n",
       " ('restrictions', 'n'),\n",
       " ('whatsoever', 'v'),\n",
       " ('may', 'n'),\n",
       " ('copy', 'v'),\n",
       " ('give', 'v'),\n",
       " ('away', 'r'),\n",
       " ('use', 'n'),\n",
       " ('terms', 'n'),\n",
       " ('license', 'n'),\n",
       " ('included', 'v'),\n",
       " ('online', 'a'),\n",
       " ('www', 'a'),\n",
       " ('net', 'n'),\n",
       " ('copyrighted', 'v'),\n",
       " ('details', 'n'),\n",
       " ('please', 'v'),\n",
       " ('follow', 'a'),\n",
       " ('guidelines', 'n'),\n",
       " ('file', 'n'),\n",
       " ('title', 'n'),\n",
       " ('metamorphosis', 'n'),\n",
       " ('author', 'n'),\n",
       " ('franz', 'n'),\n",
       " ('kafka', 'n'),\n",
       " ('translator', 'n'),\n",
       " ('david', 'n'),\n",
       " ('wyllie', 'n'),\n",
       " ('release', 'n'),\n",
       " ('date', 'n'),\n",
       " ('august', 'n'),\n",
       " ('first', 'r'),\n",
       " ('posted', 'v'),\n",
       " ('may', 'n'),\n",
       " ('last', 'a'),\n",
       " ('updated', 'a'),\n",
       " ('may', 'n'),\n",
       " ('language', 'n'),\n",
       " ('english', 'v'),\n",
       " ('start', 'a'),\n",
       " ('metamorphosis', 'n'),\n",
       " ('c', 'n'),\n",
       " ('david', 'n'),\n",
       " ('wyllie', 'n'),\n",
       " ('metamorphosis', 'n'),\n",
       " ('franz', 'n'),\n",
       " ('kafka', 'n'),\n",
       " ('translated', 'v'),\n",
       " ('david', 'a'),\n",
       " ('wyllie', 'n'),\n",
       " ('one', 'n'),\n",
       " ('morning', 'n'),\n",
       " ('gregor', 'n'),\n",
       " ('samsa', 'n'),\n",
       " ('woke', 'v'),\n",
       " ('troubled', 'a'),\n",
       " ('dreams', 'n'),\n",
       " ('found', 'v'),\n",
       " ('transformed', 'v'),\n",
       " ('bed', 'n'),\n",
       " ('horrible', 'a'),\n",
       " ('vermin', 'n'),\n",
       " ('lay', 'v'),\n",
       " ('armour', 'n'),\n",
       " ('like', 'n'),\n",
       " ('back', 'r'),\n",
       " ('lifted', 'v'),\n",
       " ('head', 'n'),\n",
       " ('little', 'n'),\n",
       " ('could', 'n'),\n",
       " ('see', 'v'),\n",
       " ('brown', 'a'),\n",
       " ('belly', 'r'),\n",
       " ('slightly', 'r'),\n",
       " ('domed', 'v'),\n",
       " ('divided', 'a'),\n",
       " ('arches', 'n'),\n",
       " ('stiff', 'a'),\n",
       " ('sections', 'n'),\n",
       " ('bedding', 'v'),\n",
       " ('hardly', 'r'),\n",
       " ('able', 'a'),\n",
       " ('cover', 'n'),\n",
       " ('seemed', 'v'),\n",
       " ('ready', 'a'),\n",
       " ('slide', 'a'),\n",
       " ('moment', 'n'),\n",
       " ('many', 'a'),\n",
       " ('legs', 'n'),\n",
       " ('pitifully', 'r')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metamorph_lemmas_pos[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[POS_tag](https://www.guru99.com/pos-tagging-chunking-nltk.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Lemmatizer on Metamorphosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('thin', 'thin')\n",
      "('compared', 'compare')\n",
      "('size', 'size')\n",
      "('rest', 'rest')\n",
      "('waved', 'wave')\n",
      "('helplessly', 'helplessly')\n",
      "('looked', 'look')\n",
      "('whats', 'whats')\n",
      "('happened', 'happen')\n",
      "('thought', 'think')\n",
      "('wasnt', 'wasnt')\n",
      "('dream', 'dream')\n",
      "('room', 'room')\n",
      "('proper', 'proper')\n",
      "('human', 'human')\n",
      "('room', 'room')\n",
      "('although', 'although')\n",
      "('little', 'little')\n",
      "('small', 'small')\n",
      "('lay', 'lay')\n",
      "('peacefully', 'peacefully')\n",
      "('four', 'four')\n",
      "('familiar', 'familiar')\n",
      "('walls', 'wall')\n",
      "('collection', 'collection')\n",
      "('textile', 'textile')\n",
      "('samples', 'sample')\n",
      "('lay', 'lay')\n",
      "('spread', 'spread')\n",
      "('table', 'table')\n",
      "('samsa', 'samsa')\n",
      "('travelling', 'travel')\n",
      "('salesman', 'salesman')\n",
      "('hung', 'hung')\n",
      "('picture', 'picture')\n",
      "('recently', 'recently')\n",
      "('cut', 'cut')\n",
      "('illustrated', 'illustrated')\n",
      "('magazine', 'magazine')\n",
      "('housed', 'house')\n",
      "('nice', 'nice')\n",
      "('gilded', 'gild')\n",
      "('frame', 'frame')\n",
      "('showed', 'show')\n",
      "('lady', 'lady')\n",
      "('fitted', 'fit')\n",
      "('fur', 'fur')\n",
      "('hat', 'hat')\n",
      "('fur', 'fur')\n",
      "('boa', 'boa')\n",
      "('sat', 'sit')\n",
      "('upright', 'upright')\n",
      "('raising', 'raise')\n",
      "('heavy', 'heavy')\n",
      "('fur', 'fur')\n",
      "('muff', 'muff')\n",
      "('covered', 'cover')\n",
      "('whole', 'whole')\n",
      "('lower', 'low')\n",
      "('arm', 'arm')\n",
      "('towards', 'towards')\n",
      "('viewer', 'viewer')\n",
      "('gregor', 'gregor')\n",
      "('turned', 'turn')\n",
      "('look', 'look')\n",
      "('window', 'window')\n",
      "('dull', 'dull')\n",
      "('weather', 'weather')\n",
      "('drops', 'drop')\n",
      "('rain', 'rain')\n",
      "('could', 'could')\n",
      "('heard', 'hear')\n",
      "('hitting', 'hit')\n",
      "('pane', 'pane')\n",
      "('made', 'make')\n",
      "('feel', 'feel')\n",
      "('quite', 'quite')\n",
      "('sad', 'sad')\n",
      "('sleep', 'sleep')\n",
      "('little', 'little')\n",
      "('bit', 'bit')\n",
      "('longer', 'longer')\n",
      "('forget', 'forget')\n",
      "('nonsense', 'nonsense')\n",
      "('thought', 'thought')\n",
      "('something', 'something')\n",
      "('unable', 'unable')\n",
      "('used', 'use')\n",
      "('sleeping', 'sleep')\n",
      "('right', 'right')\n",
      "('present', 'present')\n",
      "('state', 'state')\n",
      "('couldnt', 'couldnt')\n",
      "('get', 'get')\n",
      "('position', 'position')\n",
      "('however', 'however')\n",
      "('hard', 'hard')\n",
      "('threw', 'throw')\n",
      "('onto', 'onto')\n",
      "('right', 'right')\n"
     ]
    }
   ],
   "source": [
    "meta_lemmaed = []\n",
    "for word, pos in metamorph_lemmas_pos:\n",
    "    meta_lemmaed.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "print(*zip(metamorph_tokens_stopped[100:200], meta_lemmaed[100:200]), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a short list of additional considerations when cleaning text:\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_freqdist = FreqDist(meta_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gregor', 298),\n",
       " ('would', 187),\n",
       " ('room', 133),\n",
       " ('could', 120),\n",
       " ('work', 114),\n",
       " ('even', 104),\n",
       " ('father', 102),\n",
       " ('sister', 101),\n",
       " ('door', 97),\n",
       " ('mother', 90),\n",
       " ('back', 83),\n",
       " ('one', 76),\n",
       " ('time', 74),\n",
       " ('way', 66),\n",
       " ('look', 61),\n",
       " ('open', 56),\n",
       " ('tm', 56),\n",
       " ('use', 55),\n",
       " ('get', 52),\n",
       " ('said', 51),\n",
       " ('littl', 49),\n",
       " ('go', 49),\n",
       " ('without', 47),\n",
       " ('first', 45),\n",
       " ('still', 45),\n",
       " ('want', 44),\n",
       " ('like', 43),\n",
       " ('see', 42),\n",
       " ('hand', 41),\n",
       " ('made', 40),\n",
       " ('make', 40),\n",
       " ('head', 39),\n",
       " ('much', 39),\n",
       " ('come', 39),\n",
       " ('day', 38),\n",
       " ('thing', 38),\n",
       " ('move', 38),\n",
       " ('chief', 38),\n",
       " ('thought', 37),\n",
       " ('clerk', 37),\n",
       " ('turn', 36),\n",
       " ('away', 35),\n",
       " ('samsa', 34),\n",
       " ('let', 33),\n",
       " ('well', 33),\n",
       " ('bed', 32),\n",
       " ('went', 32),\n",
       " ('famili', 32),\n",
       " ('left', 32),\n",
       " ('seem', 31)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x134a496d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "meta_freqdist.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "## this step happens after we account for stopwords and lemmas; depending on the library...\n",
    "* we make a **Count Vector**, which is the formal term for a **bag of words**\n",
    "* we use vectors to pass text into machine learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the CountVectorizer method on 'basic_example'\n",
    "basic_example = ['The Data Scientist wants to train a machine to train machine learning models.']\n",
    "cv = CountVectorizer()\n",
    "cv.fit(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(basic_example[0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'models',\n",
       " 'scientist',\n",
       " 'the',\n",
       " 'to',\n",
       " 'train',\n",
       " 'wants']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what info can we get from cv?\n",
    "# hint -- look at the docs again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cv.transform(basic_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 2, 1, 1, 1, 2, 2, 1]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(basic_example).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = ['Data scientist discovered a brand new way.', \n",
    "           'Data scientist spent a lot time in cleaning the data', \n",
    "           'Data scientist should not peak at their test data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'brand',\n",
       " 'cleaning',\n",
       " 'data',\n",
       " 'discovered',\n",
       " 'in',\n",
       " 'lot',\n",
       " 'new',\n",
       " 'not',\n",
       " 'peak',\n",
       " 'scientist',\n",
       " 'should',\n",
       " 'spent',\n",
       " 'test',\n",
       " 'the',\n",
       " 'their',\n",
       " 'time',\n",
       " 'way']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(['But data scientist discovered the test data anyway', \"A better data scientist didn't\" ]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization allows us to compare two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to help see what's happening\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit the CountVectorizer on the 'basic_example', now we transform 'basic_example'\n",
    "example_vector_doc_1 = cv.transform(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# what is the type\n",
    "\n",
    "print(type(example_vector_doc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 14)\t1\n"
     ]
    }
   ],
   "source": [
    "# what does it look like\n",
    "\n",
    "print(example_vector_doc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>brand</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>data</th>\n",
       "      <th>discovered</th>\n",
       "      <th>in</th>\n",
       "      <th>lot</th>\n",
       "      <th>new</th>\n",
       "      <th>not</th>\n",
       "      <th>peak</th>\n",
       "      <th>scientist</th>\n",
       "      <th>should</th>\n",
       "      <th>spent</th>\n",
       "      <th>test</th>\n",
       "      <th>the</th>\n",
       "      <th>their</th>\n",
       "      <th>time</th>\n",
       "      <th>way</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   at  brand  cleaning  data  discovered  in  lot  new  not  peak  scientist  \\\n",
       "0   0      0         0     1           0   0    0    0    0     0          1   \n",
       "\n",
       "   should  spent  test  the  their  time  way  \n",
       "0       0      0     0    1      0     0    0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "example_vector_df = pd.DataFrame(example_vector_doc_1.toarray(), columns=cv.get_feature_names())\n",
    "example_vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>brand</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>data</th>\n",
       "      <th>discovered</th>\n",
       "      <th>in</th>\n",
       "      <th>lot</th>\n",
       "      <th>new</th>\n",
       "      <th>not</th>\n",
       "      <th>peak</th>\n",
       "      <th>scientist</th>\n",
       "      <th>should</th>\n",
       "      <th>spent</th>\n",
       "      <th>test</th>\n",
       "      <th>the</th>\n",
       "      <th>their</th>\n",
       "      <th>time</th>\n",
       "      <th>way</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   at  brand  cleaning  data  discovered  in  lot  new  not  peak  scientist  \\\n",
       "0   0      0         0     1           0   0    0    0    0     0          1   \n",
       "\n",
       "   should  spent  test  the  their  time  way  \n",
       "0       0      0     0    2      0     0    0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we compare new text to the CountVectorizer fit on 'basic_example'\n",
    "new_text = ['the data scientist plotted the residual error of her model']\n",
    "new_data = cv.transform(new_text)\n",
    "new_count = pd.DataFrame(new_data.toarray(),columns=cv.get_feature_names())\n",
    "new_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this the object 'sentences' becomes the corpus\n",
    "sentences = ['The Data Scientist wants to train a machine to train machine learning models.',\n",
    "             'the data scientist plotted the residual error of her model in her analysis',\n",
    "             'Her analysis was so good, she won a Kaggle competition.',\n",
    "             'The machine gained sentience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to the docs for count vectorizer, how would we use an ngram\n",
    "# pro tip -- include stop words\n",
    "bigrams = CountVectorizer(ngram_range=(1, 2), max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x55 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 65 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vector = bigrams.fit_transform(sentences)\n",
    "bigram_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 55 features for this corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['analysis',\n",
       " 'analysis was',\n",
       " 'competition',\n",
       " 'data',\n",
       " 'data scientist',\n",
       " 'error',\n",
       " 'error of',\n",
       " 'gained',\n",
       " 'gained sentience',\n",
       " 'good',\n",
       " 'good she',\n",
       " 'her',\n",
       " 'her analysis',\n",
       " 'her model',\n",
       " 'in',\n",
       " 'in her',\n",
       " 'kaggle',\n",
       " 'kaggle competition',\n",
       " 'learning',\n",
       " 'learning models',\n",
       " 'machine',\n",
       " 'machine gained',\n",
       " 'machine learning',\n",
       " 'machine to',\n",
       " 'model',\n",
       " 'model in']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {str(len(bigrams.get_feature_names()))} features for this corpus')\n",
    "bigrams.get_feature_names()[:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analysis was</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>error</th>\n",
       "      <th>error of</th>\n",
       "      <th>gained</th>\n",
       "      <th>gained sentience</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>to train</th>\n",
       "      <th>train</th>\n",
       "      <th>train machine</th>\n",
       "      <th>wants</th>\n",
       "      <th>wants to</th>\n",
       "      <th>was</th>\n",
       "      <th>was so</th>\n",
       "      <th>won</th>\n",
       "      <th>won kaggle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analysis was  competition  data  data scientist  error  error of  \\\n",
       "0         0             0            0     1               1      0         0   \n",
       "1         1             0            0     1               1      1         1   \n",
       "2         1             1            1     0               0      0         0   \n",
       "3         0             0            0     0               0      0         0   \n",
       "\n",
       "   gained  gained sentience  good  ...  to  to train  train  train machine  \\\n",
       "0       0                 0     0  ...   2         2      2              2   \n",
       "1       0                 0     0  ...   0         0      0              0   \n",
       "2       0                 0     1  ...   0         0      0              0   \n",
       "3       1                 1     0  ...   0         0      0              0   \n",
       "\n",
       "   wants  wants to  was  was so  won  won kaggle  \n",
       "0      1         1    0       0    0           0  \n",
       "1      0         0    0       0    0           0  \n",
       "2      0         0    1       1    1           1  \n",
       "3      0         0    0       0    0           0  \n",
       "\n",
       "[4 rows x 55 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "bigram_df = pd.DataFrame(bigram_vector.toarray(), columns=bigrams.get_feature_names())\n",
    "bigram_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "## Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'the brown cow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing} i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_sentences = ['The data Scientist wants to train a machine to train machine learning models.',\n",
    "                    'the data scientist plotted the residual error of her model in her analysis',\n",
    "                    'Her data analysis was so good, she won a Kaggle competition.',\n",
    "                    'The data machine']\n",
    "# take out stop words\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "# fit transform the sentences\n",
    "tfidf_sentences = tfidf.fit_transform(tf_idf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize it\n",
    "tfidf_df = pd.DataFrame(tfidf_sentences.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>good</th>\n",
       "      <th>kaggle</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>model</th>\n",
       "      <th>models</th>\n",
       "      <th>plotted</th>\n",
       "      <th>residual</th>\n",
       "      <th>scientist</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310381</td>\n",
       "      <td>0.489416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244708</td>\n",
       "      <td>0.620762</td>\n",
       "      <td>0.310381</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222201</td>\n",
       "      <td>0.425802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425802</td>\n",
       "      <td>0.425802</td>\n",
       "      <td>0.335707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.356389</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.235891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  competition      data     error      good    kaggle  learning  \\\n",
       "0  0.000000     0.000000  0.161970  0.000000  0.000000  0.000000  0.310381   \n",
       "1  0.335707     0.000000  0.222201  0.425802  0.000000  0.000000  0.000000   \n",
       "2  0.356389     0.452035  0.235891  0.000000  0.452035  0.452035  0.000000   \n",
       "3  0.000000     0.000000  0.551939  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    machine     model    models   plotted  residual  scientist     train  \\\n",
       "0  0.489416  0.000000  0.310381  0.000000  0.000000   0.244708  0.620762   \n",
       "1  0.000000  0.425802  0.000000  0.425802  0.425802   0.335707  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "3  0.833884  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "\n",
       "      wants       won  \n",
       "0  0.310381  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.452035  \n",
       "3  0.000000  0.000000  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analysis was</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>error</th>\n",
       "      <th>error of</th>\n",
       "      <th>gained</th>\n",
       "      <th>gained sentience</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>to train</th>\n",
       "      <th>train</th>\n",
       "      <th>train machine</th>\n",
       "      <th>wants</th>\n",
       "      <th>wants to</th>\n",
       "      <th>was</th>\n",
       "      <th>was so</th>\n",
       "      <th>won</th>\n",
       "      <th>won kaggle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analysis was  competition  data  data scientist  error  error of  \\\n",
       "0         0             0            0     1               1      0         0   \n",
       "1         1             0            0     1               1      1         1   \n",
       "2         1             1            1     0               0      0         0   \n",
       "3         0             0            0     0               0      0         0   \n",
       "\n",
       "   gained  gained sentience  good  ...  to  to train  train  train machine  \\\n",
       "0       0                 0     0  ...   2         2      2              2   \n",
       "1       0                 0     0  ...   0         0      0              0   \n",
       "2       0                 0     1  ...   0         0      0              0   \n",
       "3       1                 1     0  ...   0         0      0              0   \n",
       "\n",
       "   wants  wants to  was  was so  won  won kaggle  \n",
       "0      1         1    0       0    0           0  \n",
       "1      0         0    0       0    0           0  \n",
       "2      0         0    1       1    1           1  \n",
       "3      0         0    0       0    0           0  \n",
       "\n",
       "[4 rows x 55 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared to bigrams\n",
    "bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test out our TfidfVectorizer\n",
    "test_tdidf = tfidf.transform(['this is a test document','look at me I am a test document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x16 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a vector\n",
    "test_tdidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>good</th>\n",
       "      <th>kaggle</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>model</th>\n",
       "      <th>models</th>\n",
       "      <th>plotted</th>\n",
       "      <th>residual</th>\n",
       "      <th>scientist</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  competition  data  error  good  kaggle  learning  machine  model  \\\n",
       "0       0.0          0.0   0.0    0.0   0.0     0.0       0.0      0.0    0.0   \n",
       "1       0.0          0.0   0.0    0.0   0.0     0.0       0.0      0.0    0.0   \n",
       "\n",
       "   models  plotted  residual  scientist  train  wants  won  \n",
       "0     0.0      0.0       0.0        0.0    0.0    0.0  0.0  \n",
       "1     0.0      0.0       0.0        0.0    0.0    0.0  0.0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf_df = pd.DataFrame(test_tdidf.toarray(), columns=tfidf.get_feature_names())\n",
    "test_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Similarity Between Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell how similar two documents are to one another, normalizing for size, by taking the cosine similarity of the two. \n",
    "\n",
    "This number will range from [0,1], with 0 being not similar whatsoever, and 1 being the exact same. A potential application of cosine similarity is a basic recommendation engine. If you wanted to recommend articles that are most similar to other articles, you could talk the cosine similarity of all articles and return the highest one.\n",
    "\n",
    "<img src=\"./img/better_cos_similarity.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = CountVectorizer()\n",
    "sunday_afternoon = ['I ate a burger at burger queen and it was very good.',\n",
    "                    'I ate a hot dog at burger prince and it was bad',\n",
    "                    'I drove a racecar through your kitchen door',\n",
    "                    'I ate a hot dog at burger king and it was bad. I ate a burger at burger queen and it was very good']\n",
    "\n",
    "trial.fit(sunday_afternoon)\n",
    "text_data = trial.transform(sunday_afternoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# the 0th and 2nd index lines are very different, a number close to 0\n",
    "cosine_similarity(text_data[0],text_data[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 0th and 3rd index lines are very similar, despite different lengths\n",
    "cosine_similarity(text_data[1],text_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
